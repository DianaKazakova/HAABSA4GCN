{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65b05c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bromi\\anaconda3\\envs\\trigcn\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import argparse\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "from time import strftime, localtime\n",
    "import random\n",
    "import numpy as np\n",
    "import subprocess\n",
    "\n",
    "from pytorch_transformers import BertModel, BertConfig\n",
    "# from data_utils import Tokenizer4Bert, ABSADataset\n",
    "# from asa_tgcn_model import AsaTgcn\n",
    "\n",
    "# !pip install scikit-learn\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "\n",
    "CONFIG_NAME = 'config.json'\n",
    "WEIGHTS_NAME = 'pytorch_model.bin'\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02abd94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "# !pip install pytorch_transformers\n",
    "from pytorch_transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08d48c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_and_truncate(sequence, maxlen, dtype='int64', padding='post', truncating='post', value=0):\n",
    "#is designed to adjust the length of a sequence (such as a list or array) to a specified maximum length\n",
    "#by either padding or truncating the sequence as necessary\n",
    "#padding='post' add words in the end of the sentence if necessary \n",
    "#trancating='post' trancate sentence in the end if the length of a sentence is longer than maxlen\n",
    "    \n",
    "    x = (np.ones(maxlen) * value).astype(dtype)\n",
    "    \n",
    "    if truncating == 'pre':\n",
    "        trunc = sequence[-maxlen:] \n",
    "    else:\n",
    "        trunc = sequence[:maxlen]\n",
    "    \n",
    "    trunc = np.asarray(trunc, dtype=dtype)\n",
    "\n",
    "    if padding == 'post':\n",
    "        x[:len(trunc)] = trunc\n",
    "    else:\n",
    "        x[-len(trunc):] = trunc\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "604237fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args(model_type = 'tgcn', # tgcn, tgcn+sem, tri_gcn\n",
    "             # Select which modules to use for hybrid model\n",
    "             tgcn = True,\n",
    "             semgcn = True, \n",
    "             lexgcn = True,\n",
    "             tgcn_layers = 3,\n",
    "             semgcn_layers = 2,\n",
    "             lexgcn_layers = 2,\n",
    "             path = None, \n",
    "             year='2015',\n",
    "             val_file='val.txt',\n",
    "             log = 'log',\n",
    "             bert_model='bert-large-uncased', # ADD ANOTHER VALUE FOR THAT VARIABLE BECAUSE OF THE ERROR (change underscore to the dash)\n",
    "             #bert_model='bert_large_uncased',\n",
    "             cooc_path = 'cooc_matrix.csv', # Path to co-occurrence matrix file\n",
    "             cooc = None, # Pandas DataFrame co-occurrence matrix. If not specified, it will be loaded from cooc_path\n",
    "             learning_rate=2e-5,\n",
    "             dropout=0.2,\n",
    "             concat_dropout = 0.5,\n",
    "             bert_dropout=0.2,\n",
    "             l2reg=0.01,\n",
    "             num_epoch=50,\n",
    "             batch_size=6, # PREVIOUS NUMBER 16  CHANGED BECAUSE OF THE MEMORY ERROR\n",
    "             log_step=5,\n",
    "             max_seq_len=100,\n",
    "             polarities_dim=3,\n",
    "             device='cuda',\n",
    "             seed=50,\n",
    "             valset_ratio=0.1, # the percentage fo the validation set\n",
    "             do_train=True,\n",
    "             do_eval=True,\n",
    "             eval_epoch_num=0,\n",
    "             fusion_type = 'concat', # 'concat' or 'gate'\n",
    "             use_ensemble = True, \n",
    "            save_models='last',\n",
    "            print_sentences = False,\n",
    "             optim = 'adam'\n",
    "            ):\n",
    "    assert model_type == 'tgcn' or model_type == 'tgcn+sem' or model_type == 'tri_gcn'\n",
    "    opt = argparse.Namespace()\n",
    "    opt.model_type = model_type\n",
    "    opt.modules = {'tgcn': tgcn, 'semgcn': semgcn, 'lexgcn': lexgcn}\n",
    "    opt.num_layers = {'tgcn': tgcn_layers, 'semgcn': semgcn_layers, 'lexgcn': lexgcn_layers}\n",
    "    opt.year = year\n",
    "    fusion = \"\" if model_type == 'tgcn' else \"+\" + fusion_type\n",
    "    opt.train_file = f'data/train{year}restaurant.txt'\n",
    "    opt.test_file = f'data/test{year}restaurant.txt'\n",
    "    opt.model_path = f'test_models/{year}{model_type}{fusion}_seed{seed}_reg{l2reg}_drop{dropout}_cdrop{concat_dropout}_lr{learning_rate}_epochs{num_epoch}_{optim.lower()}'\n",
    "#     if model_type == 'tgcn':\n",
    "#         opt.model_path = f'models/rest_{year}/BERT.L_seed{seed}_reg{l2reg}_drop{dropout}_lr{learning_rate}_epochs{num_epoch}' \n",
    "#     elif model_type == 'tgcn+sem':\n",
    "#         opt.model_path = f'models/rest_{year}/{model_type}/{model_type}_seed{seed}_reg{l2reg}_drop{dropout}_lr{learning_rate}_epochs{num_epoch}'\n",
    "    if do_eval and not do_train:\n",
    "        opt.model_path += f'/epoch_{eval_epoch_num}'\n",
    "    if path:\n",
    "        opt.model_path = path\n",
    "    opt.val_file = val_file\n",
    "    opt.log = log\n",
    "    opt.bert_model = bert_model\n",
    "    opt.cooc_path = cooc_path\n",
    "    opt.cooc = cooc\n",
    "    opt.learning_rate = learning_rate\n",
    "    opt.dropout = dropout\n",
    "    opt.concat_dropout = concat_dropout\n",
    "    opt.bert_dropout = bert_dropout\n",
    "    opt.l2reg = l2reg\n",
    "    opt.num_epoch = num_epoch\n",
    "    opt.batch_size = batch_size\n",
    "    opt.log_step = log_step\n",
    "    opt.max_seq_len = max_seq_len\n",
    "    opt.polarities_dim = polarities_dim\n",
    "    opt.device = device\n",
    "    opt.seed = seed\n",
    "    opt.valset_ratio = valset_ratio\n",
    "    opt.do_train = do_train\n",
    "    opt.do_eval = do_eval\n",
    "    opt.eval_epoch_num = eval_epoch_num\n",
    "    opt.fusion_type = fusion_type\n",
    "    opt.use_ensemble = True\n",
    "    opt.save_models = save_models\n",
    "    opt.print_sent = print_sentences\n",
    "    opt.optim = optim\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3bde19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer4Bert:\n",
    "#is designed to handle the tokenization of text for use with a BERT model\n",
    "    def __init__(self, max_seq_len, pretrained_bert_name):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(pretrained_bert_name)\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def text_to_sequence(self, text, reverse=False, padding='post', truncating='post'):\n",
    "        sequence = self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(text)) \n",
    "        #tokenize the text and thenmap it with the corresponding ids\n",
    "        if len(sequence) == 0:\n",
    "            sequence = [0]\n",
    "        if reverse:\n",
    "            sequence = sequence[::-1]\n",
    "        return pad_and_truncate(sequence, self.max_seq_len, padding=padding, truncating=truncating)\n",
    "\n",
    "    def id_to_sequence(self, sequence, reverse=False, padding='post', truncating='post'):\n",
    "        if len(sequence) == 0:\n",
    "            sequence = [0]\n",
    "        if reverse:\n",
    "            sequence = sequence[::-1]\n",
    "        return pad_and_truncate(sequence, self.max_seq_len, padding=padding, truncating=truncating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adaa9961",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ABSADataset(Dataset):\n",
    "    def __init__(self, datafile, tokenizer, opt, deptype2id=None, dep_order=\"first\"):\n",
    "        self.datafile = datafile\n",
    "        self.depfile = \"{}.dep\".format(datafile)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.opt = opt #IT IS THE ORIGINAL CODE, I COMMENTED BECAUSE I HAVE A MISTAKE WHEN CREATING COOC\n",
    "        #self.opt = opt if opt is not None else get_default_config()\n",
    "        self.deptype2id = deptype2id\n",
    "        self.dep_order = dep_order\n",
    "        self.textdata = ABSADataset.load_datafile(self.datafile)\n",
    "        self.depinfo = ABSADataset.load_depfile(self.depfile)\n",
    "        self.polarity2id = self.get_polarity2id()\n",
    "        self.feature = []\n",
    "        for sentence,depinfo in zip(self.textdata, self.depinfo):\n",
    "            self.feature.append(self.create_feature(sentence, depinfo, opt.print_sent))\n",
    "            \n",
    "        #self.cooc_matrix = self.create_cooc_matrix() # NEW LINE TO CREATE COOC \n",
    "        print(self.feature[:1])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.feature[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feature)\n",
    "\n",
    "    def ws(self, text):\n",
    "        tokens = []\n",
    "        valid_ids = []\n",
    "        for i, word in enumerate(text):\n",
    "            if len(text) <= 0:\n",
    "                continue\n",
    "            token = self.tokenizer.tokenizer.tokenize(word)\n",
    "            tokens.extend(token)\n",
    "            for m in range(len(token)):\n",
    "                if m == 0:\n",
    "                    valid_ids.append(1)\n",
    "                else:\n",
    "                    valid_ids.append(0)\n",
    "        token_ids = self.tokenizer.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        return tokens, token_ids, valid_ids\n",
    "\n",
    "    def create_feature(self, sentence, depinfo, print_sent = False):\n",
    "        text_left, text_right, aspect, polarity = sentence\n",
    "\n",
    "        cls_id = self.tokenizer.tokenizer.vocab[\"[CLS]\"]\n",
    "        sep_id = self.tokenizer.tokenizer.vocab[\"[SEP]\"]\n",
    "\n",
    "        doc = text_left + \" \" + aspect + \" \" + text_right\n",
    "\n",
    "        left_tokens, left_token_ids, left_valid_ids = self.ws(text_left.split(\" \"))\n",
    "        right_tokens, right_token_ids, right_valid_ids = self.ws(text_right.split(\" \"))\n",
    "        aspect_tokens, aspect_token_ids, aspect_valid_ids = self.ws(aspect.split(\" \"))\n",
    "        tokens = left_tokens + aspect_tokens + right_tokens\n",
    "        input_ids = [cls_id] + left_token_ids + aspect_token_ids + right_token_ids + [sep_id] + aspect_token_ids + [sep_id]\n",
    "        valid_ids = [1] + left_valid_ids + aspect_valid_ids + right_valid_ids + [1] + aspect_valid_ids + [1]\n",
    "        mem_valid_ids = [0] + [0] * len(left_tokens) + [1] * len(aspect_tokens) + [0] * len(right_tokens) # aspect terms mask\n",
    "        segment_ids = [0] * (len(tokens) + 2) + [1] * (len(aspect_tokens)+1)\n",
    "        \n",
    "        \n",
    "        dep_instance_parser = DepInstanceParser(basicDependencies=depinfo, tokens=[])\n",
    "        if self.dep_order == \"first\":\n",
    "            dep_adj_matrix, dep_type_matrix = dep_instance_parser.get_first_order()\n",
    "        elif self.dep_order == \"second\":\n",
    "            dep_adj_matrix, dep_type_matrix = dep_instance_parser.get_second_order()\n",
    "        elif self.dep_order == \"third\":\n",
    "            dep_adj_matrix, dep_type_matrix = dep_instance_parser.get_third_order()\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "        token_head_list = []\n",
    "        for input_id, valid_id in zip(input_ids, valid_ids):\n",
    "            if input_id == cls_id:\n",
    "                continue\n",
    "            if input_id == sep_id:\n",
    "                break\n",
    "            if valid_id == 1:\n",
    "                token_head_list.append(input_id)\n",
    "\n",
    "        input_ids = self.tokenizer.id_to_sequence(input_ids)\n",
    "        valid_ids = self.tokenizer.id_to_sequence(valid_ids)\n",
    "        segment_ids = self.tokenizer.id_to_sequence(segment_ids)\n",
    "        mem_valid_ids = self.tokenizer.id_to_sequence(mem_valid_ids)\n",
    "\n",
    "        size = input_ids.shape[0]\n",
    "        \n",
    "        if print_sent:\n",
    "            print(doc)\n",
    "            print(len(dep_adj_matrix[0]))\n",
    "\n",
    "        # final_dep_adj_matrix = [[0] * size for _ in range(self.tokenizer.max_seq_len)]\n",
    "        # final_dep_value_matrix = [[0] * size for _ in range(self.tokenizer.max_seq_len)]\n",
    "        final_dep_adj_matrix = [[0] * size for _ in range(size)]\n",
    "        final_dep_value_matrix = [[0] * size for _ in range(size)]\n",
    "        for i in range(len(token_head_list)):\n",
    "            for j in range(len(dep_adj_matrix[i])):\n",
    "                if j >= size:\n",
    "                    break\n",
    "                final_dep_adj_matrix[i+1][j] = dep_adj_matrix[i][j]\n",
    "                final_dep_value_matrix[i+1][j] = self.deptype2id[dep_type_matrix[i][j]]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\":torch.tensor(input_ids),\n",
    "            \"valid_ids\":torch.tensor(valid_ids),\n",
    "            \"segment_ids\":torch.tensor(segment_ids),\n",
    "            \"mem_valid_ids\":torch.tensor(mem_valid_ids),\n",
    "            \"dep_adj_matrix\":torch.tensor(final_dep_adj_matrix),\n",
    "            \"dep_value_matrix\":torch.tensor(final_dep_value_matrix),\n",
    "            \"polarity\": self.polarity2id[polarity],\n",
    "            \"raw_text\": doc,\n",
    "            \"aspect\": aspect\n",
    "        }\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def load_depfile(filename):\n",
    "        data = []\n",
    "        with open(filename, 'r') as f:\n",
    "            dep_info = []\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if len(line) > 0:\n",
    "                    items = line.split(\"\\t\")\n",
    "                    dep_info.append({\n",
    "                        \"governor\": int(items[0]),\n",
    "                        \"dependent\": int(items[1]),\n",
    "                        \"dep\": items[2],\n",
    "                    })\n",
    "                else:\n",
    "                    if len(dep_info) > 0:\n",
    "                        data.append(dep_info)\n",
    "                        dep_info = []\n",
    "            if len(dep_info) > 0:\n",
    "                data.append(dep_info)\n",
    "                dep_info = []\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def load_datafile(filename):\n",
    "        data = []\n",
    "        with open(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for i in range(0, len(lines), 3):\n",
    "                text_left, _, text_right = [s.lower().strip() for s in lines[i].partition(\"$T$\")]\n",
    "                aspect = lines[i + 1].lower().strip()\n",
    "                text_right = text_right.replace(\"$T$\", aspect)\n",
    "                polarity = lines[i + 2].strip()\n",
    "                data.append([text_left, text_right, aspect, polarity])\n",
    "\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def load_deptype_map(opt):\n",
    "        deptype_set = set()\n",
    "        for filename in [opt.train_file, opt.test_file, opt.val_file]:\n",
    "            filename = \"{}.dep\".format(filename)\n",
    "            if os.path.exists(filename) is False:\n",
    "                continue\n",
    "            data = ABSADataset.load_depfile(filename)\n",
    "            for dep_info in data:\n",
    "                for item in dep_info:\n",
    "                    deptype_set.add(item['dep'])\n",
    "        deptype_map = {\"none\": 0}\n",
    "        for deptype in sorted(deptype_set, key=lambda x:x):\n",
    "            deptype_map[deptype] = len(deptype_map)\n",
    "        return deptype_map\n",
    "\n",
    "    @staticmethod\n",
    "    def get_polarity2id():\n",
    "        polarity_label = [\"-1\",\"0\",\"1\"]\n",
    "        return dict([(label, idx) for idx,label in enumerate(polarity_label)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa31a6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepInstanceParser():\n",
    "    def __init__(self, basicDependencies, tokens):\n",
    "        self.basicDependencies = basicDependencies\n",
    "        self.tokens = tokens\n",
    "        self.words = []\n",
    "        self.dep_governed_info = []\n",
    "        self.dep_parsing()\n",
    "\n",
    "\n",
    "    def dep_parsing(self):\n",
    "#         print('strat dep_parsing function...')\n",
    "#         print(len(self.tokens)) #0\n",
    "        if len(self.tokens) > 0:\n",
    "            words = []\n",
    "            for token in self.tokens:\n",
    "                token['word'] = token\n",
    "                words.append(self.change_word(token['word'])) #change_word check for the paranthesis\n",
    "            dep_governed_info = [\n",
    "                {\"word\": word}\n",
    "                for i,word in enumerate(words)\n",
    "            ]\n",
    "            self.words = words\n",
    "        else:\n",
    "            dep_governed_info = [{}] * len(self.basicDependencies)\n",
    "        for dep in self.basicDependencies:\n",
    "            dependent_index = dep['dependent'] - 1\n",
    "            governed_index = dep['governor'] - 1\n",
    "            dep_governed_info[dependent_index] = {\n",
    "                \"governor\": governed_index,\n",
    "                \"dep\": dep['dep']\n",
    "            }\n",
    "        self.dep_governed_info = dep_governed_info #contains detailed information about the dependencies among these tokens.\n",
    "\n",
    "    def change_word(self, word):\n",
    "    #designed to handle specific formatting issues within the text data it processes, particularly dealing \n",
    "    #with tokens representing left and right parentheses.\n",
    "        \n",
    "        if \"-RRB-\" in word:\n",
    "        #The method first checks if the string \"-RRB-\" is present in the word. This string is often used in \n",
    "        #linguistic data to represent a right parenthesis ) to prevent misinterpretation during parsing processes. \n",
    "        #If \"-RRB-\" is found, it is replaced with \")\".\n",
    "            return word.replace(\"-RRB-\", \")\")\n",
    "        \n",
    "        if \"-LRB-\" in word:\n",
    "        #Next, the method checks for the presence of \"-LRB-\" in the word. Similarly, this string represents a left \n",
    "        #parenthesis ( and is replaced by \"(\".\n",
    "            return word.replace(\"-LRB-\", \"(\")\n",
    "        return word\n",
    "\n",
    "    def get_first_order(self, direct=False):\n",
    "        #designed to generate matrices representing the adjacency and types of dependency relationships between \n",
    "        #tokens in a sentence based on their parsed dependencies.\n",
    "        \n",
    "        #indicate whether there is a direct dependency link between the tokens\n",
    "        dep_adj_matrix  = [[0] * len(self.dep_governed_info) for _ in range(len(self.dep_governed_info))]\n",
    "        \n",
    "        #indicate the type of dependency (like \"subj\", \"obj\") between tokens instead of binary indicators as in dep_adj_matrix\n",
    "        dep_type_matrix = [[\"none\"] * len(self.dep_governed_info) for _ in range(len(self.dep_governed_info))]\n",
    "        \n",
    "        for i, dep_info in enumerate(self.dep_governed_info):\n",
    "            governor = dep_info[\"governor\"] #the index of the token that governs the current token\n",
    "            dep_type = dep_info[\"dep\"] #the type of the dependency\n",
    "            \n",
    "            #indicate the existance of the dependency between tokens\n",
    "            dep_adj_matrix[i][governor] = 1\n",
    "            dep_adj_matrix[governor][i] = 1\n",
    "            \n",
    "            #If direct is False, both [i][governor] and [governor][i] are set to the dependency type.\n",
    "            #If direct is True, the entries are suffixed to indicate the direction (_in for incoming, _out for outgoing \n",
    "            #dependencies relative to each token).\n",
    "            \n",
    "            dep_type_matrix[i][governor] = dep_type if direct is False else \"{}_in\".format(dep_type)\n",
    "            dep_type_matrix[governor][i] = dep_type if direct is False else \"{}_out\".format(dep_type)\n",
    "        \n",
    "        return dep_adj_matrix, dep_type_matrix\n",
    "\n",
    "    def get_next_order(self, dep_adj_matrix, dep_type_matrix):\n",
    "        new_dep_adj_matrix = copy.deepcopy(dep_adj_matrix)\n",
    "        new_dep_type_matrix = copy.deepcopy(dep_type_matrix)\n",
    "        for target_index in range(len(dep_adj_matrix)):\n",
    "            for first_order_index in range(len(dep_adj_matrix[target_index])):\n",
    "                if dep_adj_matrix[target_index][first_order_index] == 0:\n",
    "                    continue\n",
    "                for second_order_index in range(len(dep_adj_matrix[first_order_index])):\n",
    "                    if dep_adj_matrix[first_order_index][second_order_index] == 0:\n",
    "                        continue\n",
    "                    if second_order_index == target_index:\n",
    "                        continue\n",
    "                    if new_dep_adj_matrix[target_index][second_order_index] == 1:\n",
    "                        continue\n",
    "                    new_dep_adj_matrix[target_index][second_order_index] = 1\n",
    "                    new_dep_type_matrix[target_index][second_order_index] = dep_type_matrix[first_order_index][second_order_index]\n",
    "        return new_dep_adj_matrix, new_dep_type_matrix\n",
    "\n",
    "    def get_second_order(self, direct=False):\n",
    "        dep_adj_matrix, dep_type_matrix = self.get_first_order(direct=direct)\n",
    "        return self.get_next_order(dep_adj_matrix, dep_type_matrix)\n",
    "\n",
    "    def get_third_order(self, direct=False):\n",
    "        dep_adj_matrix, dep_type_matrix = self.get_second_order(direct=direct)\n",
    "        return self.get_next_order(dep_adj_matrix, dep_type_matrix)\n",
    "\n",
    "    def search_dep_path(self, start_idx, end_idx, adj_max, dep_path_arr):\n",
    "        for next_id in range(len(adj_max[start_idx])):\n",
    "            if next_id in dep_path_arr or adj_max[start_idx][next_id] in [\"none\"]:\n",
    "                continue\n",
    "            if next_id == end_idx:\n",
    "                return 1, dep_path_arr + [next_id]\n",
    "            stat, dep_arr = self.search_dep_path(next_id, end_idx, adj_max, dep_path_arr + [next_id])\n",
    "            if stat == 1:\n",
    "                return stat, dep_arr\n",
    "        return 0, []\n",
    "\n",
    "    def get_dep_path(self, start_index, end_index, direct=False):\n",
    "        dep_adj_matrix, dep_type_matrix = self.get_first_order(direct=direct)\n",
    "        _, dep_path = self.search_dep_path(start_index, end_index, dep_type_matrix, [start_index])\n",
    "        return dep_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84335b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt from cache at C:\\Users\\bromi\\.cache\\torch\\pytorch_transformers\\9b3c03a36e83b13d5ba95ac965c9f9074a99e14340c523ab405703179e79fc46.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "[{'input_ids': tensor([  101, 13325,  2013,  3025,  8466,  2023,  2109,  2000,  2022,  1037,\n",
      "         2204,  2173,  1010,  2021,  2025,  2151,  2936,  1012,   102,  2173,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'valid_ids': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'segment_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'mem_valid_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'dep_adj_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_value_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'polarity': 0, 'raw_text': 'judging from previous posts this used to be a good place , but not any longer .', 'aspect': 'place'}]\n"
     ]
    }
   ],
   "source": [
    "#trainset for 2015\n",
    "opt = get_args(year='2015')\n",
    "deptype2id = ABSADataset.load_deptype_map(opt)\n",
    "tokenizer = Tokenizer4Bert(opt.max_seq_len, opt.bert_model)\n",
    "trainset2015 = ABSADataset(opt.train_file, tokenizer, opt, deptype2id=deptype2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea228022",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt from cache at C:\\Users\\bromi\\.cache\\torch\\pytorch_transformers\\9b3c03a36e83b13d5ba95ac965c9f9074a99e14340c523ab405703179e79fc46.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "[{'input_ids': tensor([  101, 13325,  2013,  3025,  8466,  2023,  2109,  2000,  2022,  1037,\n",
      "         2204,  2173,  1010,  2021,  2025,  2151,  2936,  1012,   102,  2173,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'valid_ids': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'segment_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'mem_valid_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'dep_adj_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_value_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'polarity': 0, 'raw_text': 'judging from previous posts this used to be a good place , but not any longer .', 'aspect': 'place'}]\n"
     ]
    }
   ],
   "source": [
    "#trainset for 2016\n",
    "opt = get_args(year='2016')\n",
    "deptype2id = ABSADataset.load_deptype_map(opt)\n",
    "tokenizer = Tokenizer4Bert(opt.max_seq_len, opt.bert_model)\n",
    "trainset2016 = ABSADataset(opt.train_file, tokenizer, opt, deptype2id=deptype2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bd2754d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge senteces from both datasets\n",
    "cooc_text = []\n",
    "\n",
    "# Collect text from the 2015 dataset\n",
    "for feature in trainset2015.feature:\n",
    "    cooc_text.append(feature['raw_text'])\n",
    "\n",
    "# Collect text from the 2016 dataset\n",
    "for feature in trainset2016.feature:\n",
    "    cooc_text.append(feature['raw_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "598a0fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from nltk.corpus import webtext\n",
    "from collections import defaultdict\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84aac695",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package webtext to\n",
      "[nltk_data]     C:\\Users\\bromi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package webtext is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloading the webtext corpus\n",
    "nltk.download('webtext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "36826daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load webtext sentences\n",
    "web_sentences = [text for text in webtext.sents()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a8bde6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_sentences = [' '.join(sent) for sent in webtext.sents()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5c412070",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in web_sentences:\n",
    "    cooc_text.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "82edaf47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              judging          from  previous     posts          this  \\\n",
      "judging      0.000000  8.880995e-04  0.021739  0.200000  0.000000e+00   \n",
      "from         0.000888  2.488321e-12  0.000097  0.000355  2.238906e-05   \n",
      "previous     0.021739  9.653255e-05  0.000000  0.008696  3.425283e-05   \n",
      "posts        0.200000  3.552398e-04  0.008696  0.000000  2.100840e-04   \n",
      "this         0.000000  2.238906e-05  0.000034  0.000210  2.282724e-12   \n",
      "...               ...           ...       ...       ...           ...   \n",
      "persistence  0.000000  0.000000e+00  0.000000  0.000000  0.000000e+00   \n",
      "juniper      0.000000  0.000000e+00  0.000000  0.000000  0.000000e+00   \n",
      "bel          0.000000  0.000000e+00  0.000000  0.000000  0.000000e+00   \n",
      "##utz        0.000000  0.000000e+00  0.000000  0.000000  0.000000e+00   \n",
      "ideally      0.000000  0.000000e+00  0.000000  0.000000  0.000000e+00   \n",
      "\n",
      "                 used        to        be         a      good  ...  gently  \\\n",
      "judging      0.000000  0.000000  0.000000  0.000000  0.000000  ...     0.0   \n",
      "from         0.000013  0.000012  0.000008  0.000018  0.000004  ...     0.0   \n",
      "previous     0.000308  0.000018  0.000000  0.000015  0.000000  ...     0.0   \n",
      "posts        0.002837  0.000085  0.000000  0.000027  0.000000  ...     0.0   \n",
      "this         0.000030  0.000014  0.000017  0.000012  0.000011  ...     0.0   \n",
      "...               ...       ...       ...       ...       ...  ...     ...   \n",
      "persistence  0.000000  0.000000  0.000000  0.000000  0.000000  ...     0.0   \n",
      "juniper      0.000000  0.000000  0.000000  0.000000  0.000000  ...     0.0   \n",
      "bel          0.000000  0.000000  0.000000  0.000136  0.000000  ...     0.0   \n",
      "##utz        0.000000  0.000141  0.000000  0.000000  0.000000  ...     0.0   \n",
      "ideally      0.000000  0.000000  0.000000  0.000000  0.000000  ...     0.0   \n",
      "\n",
      "             weakening  slate  assistants  attributed  persistence  juniper  \\\n",
      "judging            0.0    0.0    0.000000         0.0          0.0      0.0   \n",
      "from               0.0    0.0    0.000000         0.0          0.0      0.0   \n",
      "previous           0.0    0.0    0.000000         0.0          0.0      0.0   \n",
      "posts              0.0    0.0    0.000000         0.0          0.0      0.0   \n",
      "this               0.0    0.0    0.000525         0.0          0.0      0.0   \n",
      "...                ...    ...         ...         ...          ...      ...   \n",
      "persistence        0.0    0.0    0.000000         0.0          0.0      0.0   \n",
      "juniper            0.0    0.0    0.000000         0.0          0.0      0.0   \n",
      "bel                0.0    0.0    0.000000         0.0          0.0      0.0   \n",
      "##utz              0.0    0.0    0.000000         0.0          0.0      0.0   \n",
      "ideally            0.0    0.0    0.000000         0.0          0.0      0.0   \n",
      "\n",
      "             bel  ##utz  ideally  \n",
      "judging      0.0    0.0      0.0  \n",
      "from         0.0    0.0      0.0  \n",
      "previous     0.0    0.0      0.0  \n",
      "posts        0.0    0.0      0.0  \n",
      "this         0.0    0.0      0.0  \n",
      "...          ...    ...      ...  \n",
      "persistence  0.0    0.0      0.0  \n",
      "juniper      0.0    0.0      0.0  \n",
      "bel          0.0    0.0      0.0  \n",
      "##utz        0.0    0.0      0.0  \n",
      "ideally      0.0    0.0      0.0  \n",
      "\n",
      "[13196 rows x 13196 columns]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "def build_cooccurrence_matrix(sentences, window_size=3):\n",
    "    cooc_counts = defaultdict(int)\n",
    "    word_counts = defaultdict(int)\n",
    "    \n",
    "    # Process each sentence\n",
    "    for sentence in sentences:\n",
    "        # Clean and tokenize the sentence\n",
    "        words = tokenizer.tokenizer.tokenize(sentence)  # sentence.split()\n",
    "        \n",
    "        # Keep track of encountered word pairs in each sentence\n",
    "        encountered_pairs = set()\n",
    "        \n",
    "        # Count word occurrences and co-occurrences within the window size\n",
    "        for i in range(len(words)):\n",
    "            # Count each word occurrence\n",
    "            word_counts[words[i]] += 1 \n",
    "            \n",
    "            start = max(0, i - window_size)\n",
    "            end = min(len(words), i + window_size + 1)\n",
    "            \n",
    "            for j in range(start, end):\n",
    "                if i != j:\n",
    "                    # Check if the word pair has already been encountered in this sentence\n",
    "                    pair = tuple(sorted([words[i], words[j]]))\n",
    "                    if pair not in encountered_pairs:\n",
    "                        # Increment co-occurrence count for this word pair\n",
    "                        cooc_counts[pair] += 1\n",
    "                        encountered_pairs.add(pair)\n",
    "\n",
    "    # Build DataFrame from co-occurrence dictionary\n",
    "    words = list(word_counts.keys())\n",
    "    cooc_matrix = pd.DataFrame(0.0, index=words, columns=words)  # Initialize DataFrame with float zeros\n",
    "    \n",
    "    for (w1, w2), count in cooc_counts.items():\n",
    "        cooc_matrix.at[w1, w2] += count\n",
    "        cooc_matrix.at[w2, w1] += count  # Assuming undirected co-occurrence\n",
    "\n",
    "    # Scale by the product of word frequencies\n",
    "    for (w1, w2), count in cooc_counts.items():\n",
    "        cooc_matrix.at[w1, w2] /= (word_counts[w1] * word_counts[w2])\n",
    "        cooc_matrix.at[w2, w1] /= (word_counts[w1] * word_counts[w2])\n",
    "\n",
    "    return cooc_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8eeadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = cooc_text\n",
    "cooc_matrix = build_cooccurrence_matrix(sentences, window_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a36e8e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "cooc_matrix_test=cooc_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "99c7c7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_index_map_2 = {tokenizer.tokenizer.convert_tokens_to_ids(tokenizer.tokenizer.tokenize(w))[0]: i for i, w in enumerate(cooc_matrix_test.columns)}\n",
    "indices_to_keep = [index for token_id, index in id_to_index_map_2.items()]\n",
    "filtered_cooc_matrix = cooc_matrix_test.iloc[indices_to_keep, indices_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "78a25849",
   "metadata": {},
   "outputs": [],
   "source": [
    "cooc_matrix_final=filtered_cooc_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "804b810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping token ids to indices of matrix\n",
    "id_to_index_map = {tokenizer.tokenizer.convert_tokens_to_ids(tokenizer.tokenizer.tokenize(w))[0]: i for i, w in enumerate(filtered_cooc_matrix.columns)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "642c52e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE CODE FOR COLUMNS AND INDEXES (COLUMNS - INDEX IN CO-OCCUR MATRIX, INDEX - INDEX IN FROM TOKENIZATION)\n",
    "cooc_ids = [token_id for token_id, index in id_to_index_map.items()]\n",
    "cooc_ids_array = np.array(cooc_ids)\n",
    "cooc_matrix_final.index = cooc_ids_array.astype(int)\n",
    "cooc_matrix_final.columns = cooc_ids_array.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c96cac41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(cooc_matrix_final)  # Convert your matrix to a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ccfcb2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('cooc_matrix_final2.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "802893f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cooc = pd.read_csv('cooc_matrix_final2.csv',index_col=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
