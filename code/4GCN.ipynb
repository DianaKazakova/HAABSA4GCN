{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f16429ef",
   "metadata": {},
   "source": [
    "# Data functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "974bb6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bromi\\anaconda3\\envs\\trigcn\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "# !pip install pytorch_transformers\n",
    "from pytorch_transformers import BertTokenizer\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3760ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_and_truncate(sequence, maxlen, dtype='int64', padding='post', truncating='post', value=0):\n",
    "#is designed to adjust the length of a sequence (such as a list or array) to a specified maximum length\n",
    "#by either padding or truncating the sequence as necessary\n",
    "#padding='post' add words in the end of the sentence if necessary \n",
    "#trancating='post' trancate sentence in the end if the length of a sentence is longer than maxlen\n",
    "    \n",
    "    x = (np.ones(maxlen) * value).astype(dtype)\n",
    "    \n",
    "    if truncating == 'pre':\n",
    "        trunc = sequence[-maxlen:] \n",
    "    else:\n",
    "        trunc = sequence[:maxlen]\n",
    "    \n",
    "    trunc = np.asarray(trunc, dtype=dtype)\n",
    "\n",
    "    if padding == 'post':\n",
    "        x[:len(trunc)] = trunc\n",
    "    else:\n",
    "        x[-len(trunc):] = trunc\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "544ff1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer4Bert:\n",
    "#is designed to handle the tokenization of text for use with a BERT model\n",
    "    def __init__(self, max_seq_len, pretrained_bert_name):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(pretrained_bert_name)\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def text_to_sequence(self, text, reverse=False, padding='post', truncating='post'):\n",
    "        sequence = self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(text)) \n",
    "        #tokenize the text and thenmap it with the corresponding ids\n",
    "        if len(sequence) == 0:\n",
    "            sequence = [0]\n",
    "        if reverse:\n",
    "            sequence = sequence[::-1]\n",
    "        return pad_and_truncate(sequence, self.max_seq_len, padding=padding, truncating=truncating)\n",
    "\n",
    "    def id_to_sequence(self, sequence, reverse=False, padding='post', truncating='post'):\n",
    "        if len(sequence) == 0:\n",
    "            sequence = [0]\n",
    "        if reverse:\n",
    "            sequence = sequence[::-1]\n",
    "        return pad_and_truncate(sequence, self.max_seq_len, padding=padding, truncating=truncating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c3f9f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepInstanceParser():\n",
    "    def __init__(self, basicDependencies, tokens):\n",
    "        self.basicDependencies = basicDependencies\n",
    "        self.tokens = tokens\n",
    "        self.words = []\n",
    "        self.dep_governed_info = []\n",
    "        self.dep_parsing()\n",
    "\n",
    "\n",
    "    def dep_parsing(self):\n",
    "        if len(self.tokens) > 0:\n",
    "            words = []\n",
    "            for token in self.tokens:\n",
    "                token['word'] = token\n",
    "                words.append(self.change_word(token['word'])) \n",
    "            dep_governed_info = [\n",
    "                {\"word\": word}\n",
    "                for i,word in enumerate(words)\n",
    "            ]\n",
    "            self.words = words\n",
    "        else:\n",
    "            dep_governed_info = [{}] * len(self.basicDependencies)\n",
    "        for dep in self.basicDependencies:\n",
    "            dependent_index = dep['dependent'] - 1\n",
    "            governed_index = dep['governor'] - 1\n",
    "            dep_governed_info[dependent_index] = {\n",
    "                \"governor\": governed_index,\n",
    "                \"dep\": dep['dep']\n",
    "            }\n",
    "        self.dep_governed_info = dep_governed_info #contains detailed information about the dependencies among these tokens.\n",
    "    def change_word(self, word):\n",
    "    #designed to handle specific formatting issues within the text data it processes, particularly dealing \n",
    "    #with tokens representing left and right parentheses.\n",
    "        \n",
    "        if \"-RRB-\" in word:\n",
    "        #The method first checks if the string \"-RRB-\" is present in the word. This string is often used in \n",
    "        #linguistic data to represent a right parenthesis ) to prevent misinterpretation during parsing processes. \n",
    "        #If \"-RRB-\" is found, it is replaced with \")\".\n",
    "            return word.replace(\"-RRB-\", \")\")\n",
    "        \n",
    "        if \"-LRB-\" in word:\n",
    "        #Next, the method checks for the presence of \"-LRB-\" in the word. Similarly, this string represents a left \n",
    "        #parenthesis ( and is replaced by \"(\".\n",
    "            return word.replace(\"-LRB-\", \"(\")\n",
    "        return word\n",
    "\n",
    "    def get_first_order(self, direct=False):\n",
    "        #designed to generate matrices representing the adjacency and types of dependency relationships between \n",
    "        #tokens in a sentence based on their parsed dependencies.\n",
    "        \n",
    "        #indicate whether there is a direct dependency link between the tokens\n",
    "        dep_adj_matrix  = [[0] * len(self.dep_governed_info) for _ in range(len(self.dep_governed_info))]\n",
    "        \n",
    "        #indicate the type of dependency (like \"subj\", \"obj\") between tokens instead of binary indicators as in dep_adj_matrix\n",
    "        dep_type_matrix = [[\"none\"] * len(self.dep_governed_info) for _ in range(len(self.dep_governed_info))]\n",
    "        \n",
    "        \n",
    "        for i, dep_info in enumerate(self.dep_governed_info):\n",
    "            governor = dep_info[\"governor\"] #the index of the token that governs the current token\n",
    "            dep_type = dep_info[\"dep\"] #the type of the dependency\n",
    "            \n",
    "            #indicate the existance of the dependency between tokens\n",
    "            dep_adj_matrix[i][governor] = 1\n",
    "            dep_adj_matrix[governor][i] = 1\n",
    "            \n",
    "            #If direct is False, both [i][governor] and [governor][i] are set to the dependency type.\n",
    "            #If direct is True, the entries are suffixed to indicate the direction (_in for incoming, _out for outgoing \n",
    "            #dependencies relative to each token).\n",
    "            \n",
    "            dep_type_matrix[i][governor] = dep_type if direct is False else \"{}_in\".format(dep_type)\n",
    "            dep_type_matrix[governor][i] = dep_type if direct is False else \"{}_out\".format(dep_type)\n",
    "        \n",
    "        return dep_adj_matrix, dep_type_matrix\n",
    "\n",
    "    def get_next_order(self, dep_adj_matrix, dep_type_matrix):\n",
    "        new_dep_adj_matrix = copy.deepcopy(dep_adj_matrix)\n",
    "        new_dep_type_matrix = copy.deepcopy(dep_type_matrix)\n",
    "        for target_index in range(len(dep_adj_matrix)):\n",
    "            for first_order_index in range(len(dep_adj_matrix[target_index])):\n",
    "                if dep_adj_matrix[target_index][first_order_index] == 0:\n",
    "                    continue\n",
    "                for second_order_index in range(len(dep_adj_matrix[first_order_index])):\n",
    "                    if dep_adj_matrix[first_order_index][second_order_index] == 0:\n",
    "                        continue\n",
    "                    if second_order_index == target_index:\n",
    "                        continue\n",
    "                    if new_dep_adj_matrix[target_index][second_order_index] == 1:\n",
    "                        continue\n",
    "                    new_dep_adj_matrix[target_index][second_order_index] = 1\n",
    "                    new_dep_type_matrix[target_index][second_order_index] = dep_type_matrix[first_order_index][second_order_index]\n",
    "        return new_dep_adj_matrix, new_dep_type_matrix\n",
    "\n",
    "    def get_second_order(self, direct=False):\n",
    "        dep_adj_matrix, dep_type_matrix = self.get_first_order(direct=direct)\n",
    "        return self.get_next_order(dep_adj_matrix, dep_type_matrix)\n",
    "\n",
    "    def get_third_order(self, direct=False):\n",
    "        dep_adj_matrix, dep_type_matrix = self.get_second_order(direct=direct)\n",
    "        return self.get_next_order(dep_adj_matrix, dep_type_matrix)\n",
    "\n",
    "    def search_dep_path(self, start_idx, end_idx, adj_max, dep_path_arr):\n",
    "        for next_id in range(len(adj_max[start_idx])):\n",
    "            if next_id in dep_path_arr or adj_max[start_idx][next_id] in [\"none\"]:\n",
    "                continue\n",
    "            if next_id == end_idx:\n",
    "                return 1, dep_path_arr + [next_id]\n",
    "            stat, dep_arr = self.search_dep_path(next_id, end_idx, adj_max, dep_path_arr + [next_id])\n",
    "            if stat == 1:\n",
    "                return stat, dep_arr\n",
    "        return 0, []\n",
    "\n",
    "    def get_dep_path(self, start_index, end_index, direct=False):\n",
    "        dep_adj_matrix, dep_type_matrix = self.get_first_order(direct=direct)\n",
    "        _, dep_path = self.search_dep_path(start_index, end_index, dep_type_matrix, [start_index])\n",
    "        return dep_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7ba9d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefaultConfig:\n",
    "    def __init__(self):\n",
    "        self.print_sent = False\n",
    "        self.max_seq_len = 256 \n",
    "\n",
    "def get_default_config():\n",
    "    return DefaultConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d9959ba",
   "metadata": {
    "code_folding": [
     9,
     23,
     43
    ]
   },
   "outputs": [],
   "source": [
    "class ABSADataset(Dataset):\n",
    "    def __init__(self, datafile, tokenizer, opt, deptype2id=None, dep_order=\"first\"):\n",
    "        self.datafile = datafile\n",
    "        self.depfile = \"{}.dep\".format(datafile)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.opt = opt \n",
    "        self.deptype2id = deptype2id\n",
    "        self.dep_order = dep_order\n",
    "        self.textdata = ABSADataset.load_datafile(self.datafile)\n",
    "        self.depinfo = ABSADataset.load_depfile(self.depfile)\n",
    "        self.polarity2id = self.get_polarity2id()\n",
    "        self.feature = []\n",
    "        self.use_knogcn = opt.modules['knogcn']\n",
    "        for sentence,depinfo in zip(self.textdata, self.depinfo):\n",
    "            self.feature.append(self.create_feature(sentence, depinfo, opt.print_sent))\n",
    "            \n",
    "        #print(self.feature[:1])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.feature[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feature)\n",
    "\n",
    "    def ws(self, text):\n",
    "        tokens = []\n",
    "        valid_ids = []\n",
    "        for i, word in enumerate(text):\n",
    "            if len(text) <= 0:\n",
    "                continue\n",
    "            token = self.tokenizer.tokenizer.tokenize(word)\n",
    "            tokens.extend(token)\n",
    "            for m in range(len(token)):\n",
    "                if m == 0:\n",
    "                    valid_ids.append(1)\n",
    "                else:\n",
    "                    valid_ids.append(0)\n",
    "        token_ids = self.tokenizer.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        return tokens, token_ids, valid_ids\n",
    "\n",
    "    def create_feature(self, sentence, depinfo, print_sent = False):\n",
    "        \n",
    "        text_left, text_right, aspect, polarity = sentence\n",
    "        \n",
    "        cls_id = self.tokenizer.tokenizer.vocab[\"[CLS]\"]\n",
    "\n",
    "        sep_id = self.tokenizer.tokenizer.vocab[\"[SEP]\"]\n",
    "\n",
    "\n",
    "        doc = text_left + \" \" + aspect + \" \" + text_right\n",
    "        \n",
    "        left_tokens, left_token_ids, left_valid_ids = self.ws(text_left.split(\" \"))\n",
    "        \n",
    "        right_tokens, right_token_ids, right_valid_ids = self.ws(text_right.split(\" \"))\n",
    "        \n",
    "        aspect_tokens, aspect_token_ids, aspect_valid_ids = self.ws(aspect.split(\" \"))\n",
    "        \n",
    "        tokens = left_tokens + aspect_tokens + right_tokens\n",
    "        \n",
    "        input_ids = [cls_id] + left_token_ids + aspect_token_ids + right_token_ids + [sep_id] + aspect_token_ids + [sep_id]\n",
    "        valid_ids = [1] + left_valid_ids + aspect_valid_ids + right_valid_ids + [1] + aspect_valid_ids + [1]\n",
    "        mem_valid_ids = [0] + [0] * len(left_tokens) + [1] * len(aspect_tokens) + [0] * len(right_tokens) # aspect terms mask\n",
    "        \n",
    "        segment_ids = [0] * (len(tokens) + 2) + [1] * (len(aspect_tokens)+1)\n",
    "        \n",
    "        \n",
    "        dep_instance_parser = DepInstanceParser(basicDependencies=depinfo, tokens=[])\n",
    "        if self.dep_order == \"first\":\n",
    "            dep_adj_matrix, dep_type_matrix = dep_instance_parser.get_first_order()\n",
    "        elif self.dep_order == \"second\":\n",
    "            dep_adj_matrix, dep_type_matrix = dep_instance_parser.get_second_order()\n",
    "        elif self.dep_order == \"third\":\n",
    "            dep_adj_matrix, dep_type_matrix = dep_instance_parser.get_third_order()\n",
    "        else:\n",
    "            raise ValueError()\n",
    "        \n",
    "        \n",
    "        token_head_list = []\n",
    "        \n",
    "        for input_id, valid_id in zip(input_ids, valid_ids):\n",
    "            if input_id == cls_id:\n",
    "                continue\n",
    "            if input_id == sep_id:\n",
    "                break\n",
    "            if valid_id == 1:\n",
    "                token_head_list.append(input_id)\n",
    "                \n",
    "        dep_adj_matrix_knogcn=copy.deepcopy(dep_adj_matrix)\n",
    "        dep_type_matrix_knogcn=copy.deepcopy(dep_type_matrix)  \n",
    "        \n",
    "        if self.use_knogcn:\n",
    "            self.onto_words=onto_words\n",
    "            \n",
    "            for i in range(len(token_head_list)):\n",
    "                check=token_head_list[i] in self.onto_words\n",
    "                if not check:\n",
    "                    for j in range(len(dep_adj_matrix[i])):\n",
    "                        dep_adj_matrix_knogcn[i][j]=0\n",
    "                        dep_type_matrix_knogcn[i][j]='none'\n",
    "        \n",
    "\n",
    "        input_ids = self.tokenizer.id_to_sequence(input_ids)\n",
    "        valid_ids = self.tokenizer.id_to_sequence(valid_ids)\n",
    "        segment_ids = self.tokenizer.id_to_sequence(segment_ids)\n",
    "        mem_valid_ids = self.tokenizer.id_to_sequence(mem_valid_ids)\n",
    "       \n",
    "        size = input_ids.shape[0]\n",
    "        \n",
    "        if print_sent:\n",
    "            print(doc)\n",
    "            print(len(dep_adj_matrix[0]))\n",
    "\n",
    "\n",
    "        final_dep_adj_matrix = [[0] * size for _ in range(size)]\n",
    "        final_dep_value_matrix = [[0] * size for _ in range(size)]\n",
    "        \n",
    "\n",
    "        \n",
    "        for i in range(len(token_head_list)):\n",
    "            for j in range(len(dep_adj_matrix[i])):\n",
    "                if j >= size:\n",
    "                    break\n",
    "                final_dep_adj_matrix[i+1][j] = dep_adj_matrix[i][j]\n",
    "                final_dep_value_matrix[i+1][j] = self.deptype2id[dep_type_matrix[i][j]]\n",
    "        \n",
    "        \n",
    "        final_dep_adj_matrix_knogcn = [[0] * size for _ in range(size)]\n",
    "        final_dep_value_matrix_knogcn = [[0] * size for _ in range(size)]\n",
    "        \n",
    "        \n",
    "        for i in range(len(token_head_list)):\n",
    "            for j in range(len(dep_adj_matrix_knogcn[i])):\n",
    "                if j >= size:\n",
    "                    break\n",
    "                final_dep_adj_matrix_knogcn[i+1][j] = dep_adj_matrix_knogcn[i][j]\n",
    "                final_dep_value_matrix_knogcn[i+1][j] = self.deptype2id[dep_type_matrix_knogcn[i][j]]\n",
    "       \n",
    "        \n",
    "        return {\n",
    "            \"input_ids\":torch.tensor(input_ids),\n",
    "            \"valid_ids\":torch.tensor(valid_ids),\n",
    "            \"segment_ids\":torch.tensor(segment_ids),\n",
    "            \"mem_valid_ids\":torch.tensor(mem_valid_ids),\n",
    "            \"dep_adj_matrix\":torch.tensor(final_dep_adj_matrix),\n",
    "            \"dep_value_matrix\":torch.tensor(final_dep_value_matrix),\n",
    "            \"dep_adj_matrix_knogcn\":torch.tensor(final_dep_adj_matrix_knogcn),\n",
    "            \"dep_value_matrix_knogcn\":torch.tensor(final_dep_value_matrix_knogcn),\n",
    "            \"polarity\": self.polarity2id[polarity],\n",
    "            \"raw_text\": doc,\n",
    "            \"aspect\": aspect\n",
    "        }\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def load_depfile(filename):\n",
    "        data = []\n",
    "        with open(filename, 'r') as f:\n",
    "            dep_info = []\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if len(line) > 0:\n",
    "                    items = line.split(\"\\t\")\n",
    "                    dep_info.append({\n",
    "                        \"governor\": int(items[0]),\n",
    "                        \"dependent\": int(items[1]),\n",
    "                        \"dep\": items[2],\n",
    "                    })\n",
    "                else:\n",
    "                    if len(dep_info) > 0:\n",
    "                        data.append(dep_info)\n",
    "                        dep_info = []\n",
    "            if len(dep_info) > 0:\n",
    "                data.append(dep_info)\n",
    "                dep_info = []\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def load_datafile(filename):\n",
    "        data = []\n",
    "        with open(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for i in range(0, len(lines), 3):\n",
    "                text_left, _, text_right = [s.lower().strip() for s in lines[i].partition(\"$T$\")]\n",
    "                aspect = lines[i + 1].lower().strip()\n",
    "                text_right = text_right.replace(\"$T$\", aspect)\n",
    "                polarity = lines[i + 2].strip()\n",
    "                data.append([text_left, text_right, aspect, polarity])\n",
    "\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def load_deptype_map(opt):\n",
    "        deptype_set = set()\n",
    "        for filename in [opt.train_file, opt.test_file, opt.val_file]:\n",
    "            filename = \"{}.dep\".format(filename)\n",
    "            if os.path.exists(filename) is False:\n",
    "                continue\n",
    "            data = ABSADataset.load_depfile(filename)\n",
    "            for dep_info in data:\n",
    "                for item in dep_info:\n",
    "                    deptype_set.add(item['dep'])\n",
    "        deptype_map = {\"none\": 0}\n",
    "        for deptype in sorted(deptype_set, key=lambda x:x):\n",
    "            deptype_map[deptype] = len(deptype_map)\n",
    "        return deptype_map\n",
    "\n",
    "    @staticmethod\n",
    "    def get_polarity2id():\n",
    "        polarity_label = [\"-1\",\"0\",\"1\"]\n",
    "        return dict([(label, idx) for idx,label in enumerate(polarity_label)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6bb2ba",
   "metadata": {},
   "source": [
    "# TGCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9429738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pytorch_transformers import BertPreTrainedModel,BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58958999",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "    def forward(self, text, adj):\n",
    "        hidden = torch.matmul(text, self.weight)\n",
    "        denom = torch.sum(adj, dim=2, keepdim=True) + 1\n",
    "        output = torch.matmul(adj, hidden) / denom\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06072fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TypeGraphConvolution(nn.Module):\n",
    "    \"\"\"\n",
    "    TGCN Layer\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, embedding_dim, bias=True):\n",
    "        super(TypeGraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        self.dense = nn.Linear(embedding_dim, in_features, bias=False) \n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "    def forward(self, text, adj, dep_embed):\n",
    "        batch_size, max_len, feat_dim = text.shape \n",
    "        val_us = text.unsqueeze(dim=2) \n",
    "        val_us = val_us.repeat(1, 1, max_len, 1) \n",
    "        val_sum = val_us + self.dense(dep_embed) \n",
    "        adj_us = adj.unsqueeze(dim=-1) \n",
    "        adj_us = adj_us.repeat(1, 1, 1, feat_dim) \n",
    "        hidden = torch.matmul(val_sum, self.weight) \n",
    "        output = hidden.transpose(1,2) * adj_us \n",
    "\n",
    "        output = torch.sum(output, dim=2) \n",
    "\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99eeb68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemGraphConvolution(nn.Module):\n",
    "    \"\"\"\n",
    "    Semantic GCN layer with attention adjacency matrix \n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, attention_heads = 1, bias=True):\n",
    "        super(SemGraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "    def forward(self, text, adj):\n",
    "        hidden = torch.matmul(text, self.weight)\n",
    "        denom = torch.sum(adj, dim=2, keepdim=True) + 1\n",
    "        output = torch.matmul(adj, hidden) / denom\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5fa5991",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    it could be the functino fro the sintectic module?????\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.query = nn.Linear(input_dim, input_dim)\n",
    "        self.key = nn.Linear(input_dim, input_dim)\n",
    "        self.value = nn.Linear(input_dim, input_dim)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        queries = self.query(x)\n",
    "        keys = self.key(x)\n",
    "        values = self.value(x) \n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2)) / (self.input_dim ** 0.5) \n",
    "        attention = self.softmax(scores)\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87045287",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % h == 0 # --- devides and return the value of ther reminder --\n",
    "        #we should have a size of d_odel and h equal each other\n",
    "\n",
    "        self.d_k = d_model // h # devide with integral result -- rounds the devision\n",
    "        self.h = h\n",
    "        self.linears = self.clones(nn.Linear(d_model, d_model), 2)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, :, :query.size(1)]\n",
    "            mask = mask.unsqueeze(1)\n",
    "            \n",
    "        nbatches = query.size(0)\n",
    "        query, key = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "                             for l, x in zip(self.linears, (query, key))]\n",
    "        \n",
    "        attn = self.attention(query, key, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        return attn\n",
    "    \n",
    "\n",
    "    def attention(self, query, key, mask=None, dropout=None):\n",
    "        d_k = query.size(-1)\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "        if dropout is not None:\n",
    "            p_attn = dropout(p_attn)\n",
    "\n",
    "        return p_attn\n",
    "    \n",
    "    def clones(self, module, N):\n",
    "        return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea180f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsaTgcnSem(BertPreTrainedModel):\n",
    "    def __init__(self, config, modules, tokenizer, opt):\n",
    "#     use_ensemble = True, fusion_type = 'concat', dropout = 0.2, concat_dropout = 0.5,\n",
    "#                  cooc_path = 'cooc_matrix_ids.csv', cooc = None):\n",
    "        \"\"\"\n",
    "        modules: dictionary of form {'tgcn': bool, 'semgcn': bool, 'lexgcn': bool} # place for the new module!!!!!\n",
    "        cooc: cooc matrix as dataframe preloaded into memory. if not passed as argument,\n",
    "        the matrix will be loaded from the specified path.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(AsaTgcnSem, self).__init__(config)\n",
    "        self.opt = opt\n",
    "        self.modules = opt.modules\n",
    "        \n",
    "        \n",
    "        self.use_tgcn, self.use_semgcn, self.use_lexgcn, self.use_knogcn = opt.modules['tgcn'], opt.modules['semgcn'], opt.modules['lexgcn'], opt.modules['knogcn']\n",
    "        self.num_modules = sum((self.use_tgcn, self.use_semgcn, self.use_lexgcn, self.use_knogcn)) \n",
    "        self.use_ensemble = opt.use_ensemble\n",
    "        self.layer_number_tgcn = opt.num_layers['tgcn'] \n",
    "        self.layer_number_sem = opt.num_layers['semgcn'] \n",
    "        self.layer_number_lex = opt.num_layers['lexgcn'] \n",
    "        self.layer_number_kno = opt.num_layers['knogcn'] \n",
    "        \n",
    "\n",
    "        \n",
    "        assert self.use_tgcn or self.use_semgcn or self.use_lexgcn or self.use_knogcn \n",
    "        assert opt.fusion_type == 'concat' or opt.fusion_type == 'gate' \n",
    "        self.fusion_type = opt.fusion_type\n",
    "        \n",
    "        self.num_labels = config.num_labels \n",
    "        self.num_types = config.num_types\n",
    "        \n",
    "        \n",
    "        self.bert = BertModel(config)\n",
    "        \n",
    "\n",
    "        if self.use_tgcn:\n",
    "            self.TGCNLayers = nn.ModuleList(([TypeGraphConvolution(config.hidden_size, config.hidden_size, config.hidden_size)\n",
    "                                             for _ in range(self.layer_number_tgcn)]))\n",
    "        if self.use_semgcn:\n",
    "            self.SemGCNLayers = nn.ModuleList(([GraphConvolution(config.hidden_size, config.hidden_size)\n",
    "                                            for _ in range(self.layer_number_sem)]))\n",
    "\n",
    "            \n",
    "        if self.use_lexgcn:\n",
    "            self.LexGCNLayers = nn.ModuleList(([GraphConvolution(config.hidden_size, config.hidden_size)\n",
    "                                           for _ in range(self.layer_number_lex)]))\n",
    "\n",
    "        if self.use_knogcn:\n",
    "            self.KnoGCNLayers = nn.ModuleList(([TypeGraphConvolution(config.hidden_size, config.hidden_size, config.hidden_size)\n",
    "                                           for _ in range(self.layer_number_kno)]))\n",
    "\n",
    "\n",
    "       \n",
    "        \n",
    "        if self.use_lexgcn:\n",
    "            \n",
    "            if opt.cooc is not None: #WHERE THE PATH IS SPECIFIED FOR PRELOADED DATA\n",
    "                self.cooc = opt.cooc\n",
    "                \n",
    "            else:\n",
    "                self.cooc = pd.read_csv(opt.cooc_path, index_col=0) #WHERE THE PATH IS SPECIFIED FOR PRELOADED DATA\n",
    "                self.cooc.index = self.cooc.index.astype(int)\n",
    "                self.cooc.columns = self.cooc.columns.astype(int)\n",
    "        \n",
    "        if self.use_knogcn:\n",
    "            if opt.onto_words is not None:\n",
    "                self.onto_words = opt.onto_words\n",
    "            else:\n",
    "                self.onto_words = pd.read_csv(opt.onto_words_path) \n",
    "\n",
    "        # multiplied by two if concat\n",
    "        if self.fusion_type == 'concat':\n",
    "            self.fc_single = nn.Linear(config.hidden_size*self.num_modules, self.num_labels)\n",
    "        elif self.fusion_type == 'gate':\n",
    "            self.fc_single = nn.Linear(config.hidden_size, self.num_labels)\n",
    "        \n",
    "        #WHY MULTIPLIED BY 2\n",
    "        self.gate_weight = nn.Parameter(torch.FloatTensor(config.hidden_size, config.hidden_size * 2))\n",
    "        self.gate_bias = nn.Parameter(torch.FloatTensor(config.hidden_size))\n",
    "    \n",
    "        self.dropout = nn.Dropout(opt.dropout)\n",
    "        self.concat_dropout = nn.Dropout(opt.concat_dropout)\n",
    "        self.ensemble_linear_tgcn = nn.Linear(1, self.layer_number_tgcn)\n",
    "        self.ensemble_linear_semgcn = nn.Linear(1, self.layer_number_sem)\n",
    "        self.ensemble_linear_lexgcn = nn.Linear(1, self.layer_number_lex)\n",
    "        self.ensemble_linear_knogcn = nn.Linear(1, self.layer_number_kno)\n",
    "\n",
    "        \n",
    "        self.ensemble = nn.Parameter(torch.FloatTensor(3, 1))\n",
    "        self.dep_embedding = nn.Embedding(self.num_types, config.hidden_size, padding_idx=0)\n",
    "\n",
    "    def get_attention(self, val_out, dep_embed, adj):\n",
    "        batch_size, max_len, feat_dim = val_out.shape\n",
    "        val_us = val_out.unsqueeze(dim=2)\n",
    "        val_us = val_us.repeat(1,1,max_len,1)\n",
    "        val_cat = torch.cat((val_us, dep_embed), -1).float()\n",
    "        atten_expand = (val_cat * val_cat.transpose(1,2))\n",
    "\n",
    "        attention_score = torch.sum(atten_expand, dim=-1)\n",
    "        attention_score = attention_score / np.power(feat_dim, 0.5)\n",
    "        exp_attention_score = torch.exp(attention_score)\n",
    "        exp_attention_score = torch.mul(exp_attention_score, adj.float()) # mask\n",
    "        sum_attention_score = torch.sum(exp_attention_score, dim=-1).unsqueeze(dim=-1).repeat(1,1,max_len)\n",
    "\n",
    "        attention_score = torch.div(exp_attention_score, sum_attention_score + 1e-10)\n",
    "        if 'HalfTensor' in val_out.type():\n",
    "            attention_score = attention_score.half()\n",
    "\n",
    "        return attention_score\n",
    "    \n",
    "    def get_lex_adj(self, input_ids, batch_size, max_len):\n",
    "        # Initialize an empty adjacency tensor\n",
    "        adj_tensor = torch.zeros((batch_size, max_len, max_len))\n",
    "        \n",
    "        \n",
    "        # number of non-zero input_ids for each sentence\n",
    "        num_words = []\n",
    "        \n",
    "        # i refers to the sentence number \n",
    "        for i, id_sequence in enumerate(input_ids):\n",
    "            num_words.append(int(torch.sum(id_sequence != 0)))\n",
    "            \n",
    "            \n",
    "            for j in range(num_words[i]):\n",
    "                for k in range(num_words[i]):\n",
    "                    if j != k:\n",
    "                        id_j, id_k = id_sequence[j].item(), id_sequence[k].item()\n",
    "                        \n",
    "                        if id_j in self.cooc and id_k in self.cooc:\n",
    "                            adj_tensor[i, j, k] = self.cooc[id_j][id_k]\n",
    "                        else:\n",
    "                            adj_tensor[i, j, k] = 0\n",
    "            \n",
    "            \n",
    "        # Calculate the sums of rows for each matrix\n",
    "        row_sums = adj_tensor.sum(dim=2, keepdim=True).repeat(1, 1, max_len)\n",
    "\n",
    "        # Calculate the sums of columns for each matrix\n",
    "        column_sums = adj_tensor.sum(dim=1, keepdim=True).repeat(1, max_len, 1)\n",
    "\n",
    "        # Create a diagonal mask for each matrix\n",
    "        diagonal_mask = torch.eye(adj_tensor.size(-1)).bool().unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "\n",
    "        total_sum = row_sums + column_sums\n",
    "\n",
    "        # Set the diagonal entries to the sum of all the row and column entries (will be averaged later)\n",
    "        res = torch.where(diagonal_mask, total_sum, adj_tensor)\n",
    "        \n",
    "        adj_tensor = adj_tensor + res\n",
    "        \n",
    "        # Average \n",
    "        \n",
    "        for i, num in enumerate(num_words):\n",
    "            # Divide diagonal elements by 2\n",
    "            diagonal = torch.diagonal(adj_tensor[i])\n",
    "            diagonal_divided = diagonal / num\n",
    "\n",
    "            # Assign divided diagonal elements back to the tensor\n",
    "            adj_tensor[i].diagonal().copy_(diagonal_divided)\n",
    "\n",
    "        return adj_tensor\n",
    "\n",
    "    def get_avarage(self, aspect_indices, x):\n",
    "        aspect_indices_us = torch.unsqueeze(aspect_indices, 2)\n",
    "        x_mask = x * aspect_indices_us\n",
    "        aspect_len = (aspect_indices_us != 0).sum(dim=1)\n",
    "        x_sum = x_mask.sum(dim=1)\n",
    "        x_av = torch.div(x_sum, aspect_len)\n",
    "\n",
    "        return x_av\n",
    "    \n",
    "    def set_dropout(self, dropout):\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        \n",
    "    def forward(self, input_ids, segment_ids, valid_ids, mem_valid_ids, dep_adj_matrix, dep_value_matrix, dep_adj_matrix_knogcn,dep_value_matrix_knogcn):\n",
    "        # Generate sentence representation with BERT\n",
    "        sequence_output, pooled_output = self.bert(input_ids, segment_ids)\n",
    "        \n",
    "        # Dependency type embeddings\n",
    "        dep_embed = self.dep_embedding(dep_value_matrix)\n",
    "        dep_embed_knogcn = self.dep_embedding(dep_value_matrix_knogcn)\n",
    "        \n",
    "        \n",
    "        # Initializing valid output tensor (i.e. 0 for padding, only keeping representations of tokens in sentence)\n",
    "        batch_size, max_len, feat_dim = sequence_output.shape\n",
    "        valid_output = torch.zeros(batch_size, max_len, feat_dim, device=input_ids.device).type_as(sequence_output)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            temp = sequence_output[i][valid_ids[i] == 1]\n",
    "            valid_output[i][:temp.size(0)] = temp\n",
    "        valid_output = self.dropout(valid_output)\n",
    "\n",
    "        attention_score_for_output = [] # Useless code?\n",
    "        attention_score_knogcn_for_output=[]\n",
    "        tgcn_layer_outputs = []\n",
    "        semgcn_layer_outputs = []\n",
    "        lexgcn_layer_outputs = []\n",
    "        knogcn_layer_outputs = []\n",
    "        \n",
    "        \n",
    "        seq_out_tgcn = valid_output\n",
    "        seq_out_semgcn = valid_output\n",
    "        seq_out_lexgcn = valid_output\n",
    "        seq_out_knogcn = valid_output\n",
    "        \n",
    "        \n",
    "        if self.use_tgcn:\n",
    "            for tgcn in self.TGCNLayers:\n",
    "                # Computing attention\n",
    "                attention_score = self.get_attention(seq_out_tgcn, dep_embed, dep_adj_matrix)\n",
    "                \n",
    "                attention_score_for_output.append(attention_score) # Useless code?\n",
    "\n",
    "                # Applying GCN layer\n",
    "                seq_out = F.relu(tgcn(seq_out_tgcn, attention_score, dep_embed))\n",
    "\n",
    "                # Saving layer output to be used for layer ensemble later\n",
    "                tgcn_layer_outputs.append(seq_out_tgcn)\n",
    "                \n",
    "            # Average aspect terms for each layer and combining into list \n",
    "            tgcn_layer_outputs_pool = [self.get_avarage(mem_valid_ids, x_out) for x_out in tgcn_layer_outputs]\n",
    "\n",
    "        if self.use_semgcn:\n",
    "            for semgcn in self.SemGCNLayers:\n",
    "                # Computing attention\n",
    "                attn = MultiHeadAttention(1, feat_dim)\n",
    "                attn.to('cuda')\n",
    "                attn_tensor = attn(seq_out_semgcn, seq_out_semgcn)\n",
    "                attn_tensor = attn_tensor.squeeze(1)\n",
    "\n",
    "                # Applying GCN layer\n",
    "                seq_out_semgcn = F.relu(semgcn(seq_out_semgcn, attn_tensor))\n",
    "\n",
    "                # Saving layer output\n",
    "                semgcn_layer_outputs.append(seq_out_semgcn)\n",
    "                \n",
    "            # Average aspect terms for each layer and combining into list\n",
    "            semgcn_layer_outputs_pool = [self.get_avarage(mem_valid_ids, x_out) for x_out in semgcn_layer_outputs]\n",
    "\n",
    "        \n",
    "        if self.use_lexgcn:\n",
    "            for lexgcn in self.LexGCNLayers:\n",
    "                # Compute adjaceny matrix\n",
    "                adj_tensor = self.get_lex_adj(input_ids, batch_size, max_len)\n",
    "                adj_tensor = adj_tensor.to('cuda')\n",
    "                # Applying GCN layer\n",
    "                seq_out_lexgcn = F.relu(lexgcn(seq_out_lexgcn, adj_tensor))\n",
    "                \n",
    "                # Saving layer output\n",
    "                lexgcn_layer_outputs.append(seq_out_lexgcn)\n",
    "            \n",
    "            # Average aspect terms for each layer and combining into list\n",
    "            lexgcn_layer_outputs_pool = [self.get_avarage(mem_valid_ids, x_out) for x_out in lexgcn_layer_outputs]\n",
    "        \n",
    "        if self.use_knogcn:\n",
    "            for knogcn in self.KnoGCNLayers:\n",
    "                # Computing attention\n",
    "                attention_score_knogcn = self.get_attention(seq_out_knogcn, dep_embed_knogcn, dep_adj_matrix_knogcn)\n",
    "                attention_score_knogcn_for_output.append(attention_score_knogcn) # Useless code?\n",
    "\n",
    "                # Applying GCN layer\n",
    "                seq_out_knogcn = F.relu(knogcn(seq_out_knogcn, attention_score_knogcn, dep_embed_knogcn))\n",
    "\n",
    "                # Saving layer output to be used for layer ensemble later\n",
    "                knogcn_layer_outputs.append(seq_out_knogcn)\n",
    "                \n",
    "            # Average aspect terms for each layer and combining into list \n",
    "            knogcn_layer_outputs_pool = [self.get_avarage(mem_valid_ids, x_out) for x_out in knogcn_layer_outputs]\n",
    "            \n",
    "        all_outputs = []\n",
    "        \n",
    "        if self.use_ensemble:\n",
    "            if self.use_tgcn:\n",
    "                # Layer ensemble for tgcn\n",
    "                tgcn_pool = torch.stack(tgcn_layer_outputs_pool, -1) # stacking layer outputs \n",
    "                ensemble_tgcn = torch.matmul(tgcn_pool, F.softmax(self.ensemble_linear_tgcn.weight, dim=0))\n",
    "                ensemble_tgcn = ensemble_tgcn.squeeze(dim=-1)\n",
    "                ensemble_tgcn = self.dropout(ensemble_tgcn)\n",
    "                all_outputs.append(ensemble_tgcn)\n",
    "            \n",
    "            if self.use_semgcn:\n",
    "                # Layer ensemble for semgcn\n",
    "                semgcn_pool = torch.stack(semgcn_layer_outputs_pool, -1)\n",
    "                ensemble_semgcn = torch.matmul(semgcn_pool, F.softmax(self.ensemble_linear_semgcn.weight, dim = 0))\n",
    "                ensemble_semgcn = ensemble_semgcn.squeeze(dim=-1)\n",
    "                ensemble_semgcn = self.dropout(ensemble_semgcn)\n",
    "                all_outputs.append(ensemble_semgcn)\n",
    "            \n",
    "            if self.use_lexgcn:\n",
    "            # Layer ensemble for lexgcn\n",
    "                lexgcn_pool = torch.stack(lexgcn_layer_outputs_pool, -1)\n",
    "                ensemble_lexgcn = torch.matmul(lexgcn_pool, F.softmax(self.ensemble_linear_lexgcn.weight, dim = 0))\n",
    "                ensemble_lexgcn = ensemble_lexgcn.squeeze(dim=-1)\n",
    "                ensemble_lexgcn = self.dropout(ensemble_lexgcn)\n",
    "                all_outputs.append(ensemble_lexgcn)\n",
    "            \n",
    "            if self.use_knogcn:\n",
    "            # Layer ensemble for knogcn\n",
    "                knogcn_pool = torch.stack(knogcn_layer_outputs_pool, -1)\n",
    "                ensemble_knogcn = torch.matmul(knogcn_pool, F.softmax(self.ensemble_linear_knogcn.weight, dim = 0))\n",
    "                ensemble_knogcn = ensemble_knogcn.squeeze(dim=-1)\n",
    "                ensemble_knogcn = self.dropout(ensemble_knogcn)\n",
    "                all_outputs.append(ensemble_knogcn)\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            # Take only the last layer output\n",
    "            if self.use_tgcn:\n",
    "                ensemble_tgcn = tgcn_layer_outputs_pool[-1]\n",
    "                all_outputs.append(ensemble_tgcn)\n",
    "            if self.use_semgcn:\n",
    "                ensemble_semgcn = semgcn_layer_outputs_pool[-1]\n",
    "                all_outputs.append(ensemble_semgcn)\n",
    "            if self.use_lexgcn:\n",
    "                ensemble_lexgcn = lexgcn_layer_outputs_pool[-1]\n",
    "                all_outputs.append(ensemble_lexgcn)\n",
    "            if self.use_knogcn:\n",
    "                ensemble_knogcn = knogcn_layer_outputs_pool[-1]\n",
    "                all_outputs.append(ensemble_knogcn)\n",
    "            \n",
    "        # Stacking module outputs\n",
    "        ensemble_out = torch.cat(all_outputs, dim=1)\n",
    "        \n",
    "        \n",
    "        # gating only if 2 modules used\n",
    "        # added additional combinations of modules used\n",
    "        if self.fusion_type == 'gate' and self.num_modules == 2: \n",
    "            if self.use_tgcn and self.use_semgcn:\n",
    "                concatenated = torch.cat((ensemble_tgcn, ensemble_semgcn), dim=1) \n",
    "                g = torch.matmul(concatenated, self.gate_weight.t()) + self.gate_bias  # Compute W_g[h0 ; h1] + b_g\n",
    "                g = torch.sigmoid(g)\n",
    "                ensemble_out = g * ensemble_tgcn + (1 - g) * ensemble_semgcn\n",
    "            if self.use_tgcn and self.use_lexgcn:\n",
    "                concatenated = torch.cat((ensemble_tgcn, ensemble_lexgcn), dim=1) \n",
    "                g = torch.matmul(concatenated, self.gate_weight.t()) + self.gate_bias  # Compute W_g[h0 ; h1] + b_g\n",
    "                g = torch.sigmoid(g)\n",
    "                ensemble_out = g * ensemble_tgcn + (1 - g) * ensemble_lexgcn\n",
    "            if self.use_tgcn and self.use_knogcn:\n",
    "                concatenated = torch.cat((ensemble_tgcn, ensemble_knogcn), dim=1) \n",
    "                g = torch.matmul(concatenated, self.gate_weight.t()) + self.gate_bias  # Compute W_g[h0 ; h1] + b_g\n",
    "                g = torch.sigmoid(g)\n",
    "                ensemble_out = g * ensemble_tgcn + (1 - g) * ensemble_knogcn\n",
    "            if self.use_tgcn and self.use_knogcn:\n",
    "                concatenated = torch.cat((ensemble_tgcn, ensemble_knogcn), dim=1) \n",
    "                g = torch.matmul(concatenated, self.gate_weight.t()) + self.gate_bias  # Compute W_g[h0 ; h1] + b_g\n",
    "                g = torch.sigmoid(g)\n",
    "                ensemble_out = g * ensemble_tgcn + (1 - g) * ensemble_knogcn\n",
    "            if self.use_lexgcn and self.use_semgcn:\n",
    "                concatenated = torch.cat((ensemble_semgcn, ensemble_lexgcn), dim=1) \n",
    "                g = torch.matmul(concatenated, self.gate_weight.t()) + self.gate_bias  # Compute W_g[h0 ; h1] + b_g\n",
    "                g = torch.sigmoid(g)\n",
    "                ensemble_out = g * ensemble_semgcn + (1 - g) * ensemble_lexgcn\n",
    "            if self.use_knogcn and self.use_semgcn:\n",
    "                concatenated = torch.cat((ensemble_semgcn, ensemble_knogcn), dim=1) \n",
    "                g = torch.matmul(concatenated, self.gate_weight.t()) + self.gate_bias  # Compute W_g[h0 ; h1] + b_g\n",
    "                g = torch.sigmoid(g)\n",
    "                ensemble_out = g * ensemble_semgcn + (1 - g) * ensemble_knogcn\n",
    "            if self.use_knogcn and self.use_lexgcn:\n",
    "                concatenated = torch.cat((ensemble_lexgcn, ensemble_knogcn), dim=1) \n",
    "                g = torch.matmul(concatenated, self.gate_weight.t()) + self.gate_bias  # Compute W_g[h0 ; h1] + b_g\n",
    "                g = torch.sigmoid(g)\n",
    "                ensemble_out = g * ensemble_lexgcn + (1 - g) * ensemble_knogcn\n",
    "          \n",
    "        # Additional dropout\n",
    "        if (self.num_modules == 2 and self.fusion_type == 'concat') or self.num_modules == 4: #number of modules is changed\n",
    "            ensemble_out = self.concat_dropout(ensemble_out)\n",
    "            \n",
    "        output = self.fc_single(ensemble_out)\n",
    "        \n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09478741",
   "metadata": {
    "code_folding": [
     84
    ]
   },
   "outputs": [],
   "source": [
    "class AsaTgcn(BertPreTrainedModel):\n",
    "    \n",
    "    def __init__(self, config, dropout = 0.2):\n",
    "        super(AsaTgcn, self).__init__(config)\n",
    "        self.config = config\n",
    "        self.layer_number_tgcn = 3\n",
    "        self.num_labels = config.num_labels\n",
    "        self.num_types = config.num_types\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.TGCNLayers = nn.ModuleList(([TypeGraphConvolution(config.hidden_size, config.hidden_size, config.hidden_size)\n",
    "                                         for _ in range(self.layer_number_tgcn)]))\n",
    "        self.fc_single = nn.Linear(config.hidden_size, self.num_labels)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ensemble_linear_tgcn = nn.Linear(1, self.layer_number_tgcn)\n",
    "        self.ensemble = nn.Parameter(torch.FloatTensor(3, 1))\n",
    "        self.dep_embedding = nn.Embedding(self.num_types, config.hidden_size, padding_idx=0)\n",
    "\n",
    "    def get_attention(self, val_out, dep_embed, adj):\n",
    "        batch_size, max_len, feat_dim = val_out.shape\n",
    "        val_us = val_out.unsqueeze(dim=2)\n",
    "        val_us = val_us.repeat(1,1,max_len,1)\n",
    "        val_cat = torch.cat((val_us, dep_embed), -1).float()\n",
    "        atten_expand = (val_cat * val_cat.transpose(1,2))\n",
    "\n",
    "        attention_score = torch.sum(atten_expand, dim=-1)\n",
    "        attention_score = attention_score / np.power(feat_dim, 0.5)\n",
    "        exp_attention_score = torch.exp(attention_score)\n",
    "        exp_attention_score = torch.mul(exp_attention_score, adj.float()) # mask\n",
    "        sum_attention_score = torch.sum(exp_attention_score, dim=-1).unsqueeze(dim=-1).repeat(1,1,max_len)\n",
    "\n",
    "        attention_score = torch.div(exp_attention_score, sum_attention_score + 1e-10)\n",
    "        if 'HalfTensor' in val_out.type():\n",
    "            attention_score = attention_score.half()\n",
    "\n",
    "        return attention_score\n",
    "\n",
    "    def get_avarage(self, aspect_indices, x):\n",
    "        aspect_indices_us = torch.unsqueeze(aspect_indices, 2)\n",
    "        x_mask = x * aspect_indices_us\n",
    "        aspect_len = (aspect_indices_us != 0).sum(dim=1)\n",
    "        x_sum = x_mask.sum(dim=1)\n",
    "        x_av = torch.div(x_sum, aspect_len)\n",
    "        return x_av\n",
    "    \n",
    "    def set_dropout(self, dropout):\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_ids, segment_ids, valid_ids, mem_valid_ids, dep_adj_matrix, dep_value_matrix):\n",
    "        # Generate sentence representation with BERT\n",
    "        sequence_output, pooled_output = self.bert(input_ids, segment_ids)\n",
    "        \n",
    "        # Dependency type embeddings\n",
    "        dep_embed = self.dep_embedding(dep_value_matrix)\n",
    "        \n",
    "        # Initializing valid output tensor (i.e. 0 for padding, only keeping representations of tokens in sentence)\n",
    "        batch_size, max_len, feat_dim = sequence_output.shape\n",
    "        valid_output = torch.zeros(batch_size, max_len, feat_dim, device=input_ids.device).type_as(sequence_output)\n",
    "        for i in range(batch_size):\n",
    "            temp = sequence_output[i][valid_ids[i] == 1]\n",
    "            valid_output[i][:temp.size(0)] = temp\n",
    "        valid_output = self.dropout(valid_output)\n",
    "\n",
    "        attention_score_for_output = [] \n",
    "        tgcn_layer_outputs = []\n",
    "        semgcn_layer_outputs = []\n",
    "        seq_out_tgcn = valid_output\n",
    "        seq_out_semgcn = valid_output\n",
    "        for tgcn in self.TGCNLayers:\n",
    "            # Computing attention\n",
    "            attention_score = self.get_attention(seq_out_tgcn, dep_embed, dep_adj_matrix)\n",
    "            attention_score_for_output.append(attention_score) # Useless code?\n",
    "            \n",
    "            # Applying GCN layer\n",
    "            seq_out = F.relu(tgcn(seq_out_tgcn, attention_score, dep_embed))\n",
    "            \n",
    "            # Saving layer output to be used for layer ensemble later\n",
    "            tgcn_layer_outputs.append(seq_out_tgcn)\n",
    "        \n",
    "        # Average aspect terms for each layer and combining into list\n",
    "        tgcn_layer_outputs_pool = [self.get_avarage(mem_valid_ids, x_out) for x_out in tgcn_layer_outputs]\n",
    "        \n",
    "        # Layer ensemble for tgcn\n",
    "        tgcn_pool = torch.stack(tgcn_layer_outputs_pool, -1) # stacking layer outputs \n",
    "        ensemble_tgcn = torch.matmul(tgcn_pool, F.softmax(self.ensemble_linear_tgcn.weight, dim=0))\n",
    "        ensemble_tgcn = ensemble_tgcn.squeeze(dim=-1)\n",
    "        ensemble_tgcn = self.dropout(ensemble_tgcn)\n",
    "        \n",
    "        output = self.fc_single(ensemble_tgcn)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a68451",
   "metadata": {},
   "source": [
    "# Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98ae9d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import argparse\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "from time import strftime, localtime\n",
    "import random\n",
    "import numpy as np\n",
    "import subprocess\n",
    "\n",
    "from pytorch_transformers import BertModel, BertConfig\n",
    "# from data_utils import Tokenizer4Bert, ABSADataset\n",
    "# from asa_tgcn_model import AsaTgcn\n",
    "\n",
    "# !pip install scikit-learn\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "\n",
    "CONFIG_NAME = 'config.json'\n",
    "WEIGHTS_NAME = 'pytorch_model.bin'\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ed06cd6",
   "metadata": {
    "code_folding": [
     308
    ]
   },
   "outputs": [],
   "source": [
    "class Instructor:\n",
    "    def __init__(self, opt):\n",
    "        self.opt = opt\n",
    "        logger.info(opt)\n",
    "        deptype2id = ABSADataset.load_deptype_map(opt)\n",
    "        polarity2id = ABSADataset.get_polarity2id()\n",
    "        logger.info(deptype2id)\n",
    "        logger.info(polarity2id)\n",
    "        self.deptype2id = deptype2id\n",
    "        self.polarity2id = polarity2id\n",
    "        \n",
    "        self.vocab_path = os.path.join(opt.bert_model, 'vocab.txt')\n",
    "        self.tokenizer = Tokenizer4Bert(opt.max_seq_len, opt.bert_model)\n",
    "        config = BertConfig.from_pretrained(\"bert-large-uncased\") \n",
    "        config.num_labels=opt.polarities_dim\n",
    "        config.num_types=len(self.deptype2id)\n",
    "        logger.info(config)\n",
    "        print('check the type of the model...')\n",
    "        if opt.model_type == 'tgcn': # WHAT IS MY NAME OF THE MODEL\n",
    "            self.model = AsaTgcn.from_pretrained(opt.bert_model, config=config, dropout = opt.dropout)\n",
    "        else:\n",
    "            self.model = AsaTgcnSem.from_pretrained(opt.bert_model, config=config, modules = opt.modules,\n",
    "                                                    tokenizer = self.tokenizer, opt=self.opt) \n",
    "#                                                 use_ensemble = opt.use_ensemble,\n",
    "#                                                     fusion_type = opt.fusion_type, dropout = opt.dropout, \n",
    "#                                                     concat_dropout = opt.concat_dropout,\n",
    "#                                                    cooc_path = opt.cooc_path, cooc = opt.cooc)\n",
    "        self.model.set_dropout(opt.dropout)\n",
    "        self.model.to(opt.device)\n",
    "        \n",
    "        print('downloading the files...')\n",
    "        self.fulltrainset = ABSADataset(opt.train_file, self.tokenizer, self.opt, deptype2id=deptype2id)\n",
    "        self.trainset = ABSADataset(opt.train_file, self.tokenizer, self.opt, deptype2id=deptype2id)\n",
    "        self.testset = ABSADataset(opt.test_file, self.tokenizer, self.opt, deptype2id=deptype2id)\n",
    "        \n",
    "        print('check if the val exist...')\n",
    "        if os.path.exists(opt.val_file):\n",
    "            self.valset = ABSADataset(opt.val_file, self.tokenizer, self.opt, deptype2id=deptype2id)\n",
    "        elif opt.valset_ratio > 0:\n",
    "            valset_len = int(len(self.trainset) * opt.valset_ratio)\n",
    "            self.trainset, self.valset = random_split(self.trainset, (len(self.trainset)-valset_len, valset_len))\n",
    "        else:\n",
    "            self.valset = self.testset\n",
    "        \n",
    "        print(\"check device opt.device.type == cuda\")\n",
    "        print(\"opt.device.type == cuda\")\n",
    "        if opt.device.type == 'cuda':\n",
    "            logger.info('cuda memory allocated: {}'.format(torch.cuda.memory_allocated(device=opt.device.index)))\n",
    "\n",
    "    def _print_args(self):\n",
    "        n_trainable_params, n_nontrainable_params = 0, 0\n",
    "        for p in self.model.parameters():\n",
    "            n_params = torch.prod(torch.tensor(p.shape))\n",
    "            if p.requires_grad:\n",
    "                n_trainable_params += n_params\n",
    "            else:\n",
    "                n_nontrainable_params += n_params\n",
    "        logger.info('n_trainable_params: {0}, n_nontrainable_params: {1}'.format(n_trainable_params, n_nontrainable_params))\n",
    "        logger.info('> training arguments:')\n",
    "        for arg in vars(self.opt):\n",
    "            logger.info('>>> {0}: {1}'.format(arg, getattr(self.opt, arg)))\n",
    "\n",
    "    def _reset_params(self):\n",
    "        for child in self.model.children():\n",
    "            if type(child) != BertModel:  # skip bert params\n",
    "                for p in child.parameters():\n",
    "                    if p.requires_grad:\n",
    "                        if len(p.shape) > 1:\n",
    "                            torch.nn.init.xavier_uniform_(p)\n",
    "                        else:\n",
    "                            stdv = 1. / math.sqrt(p.shape[0])\n",
    "                            torch.nn.init.uniform_(p, a=-stdv, b=stdv)\n",
    "\n",
    "    def save_model(self, save_path, model, args):\n",
    "        print('function save_model starts...')\n",
    "        # Save a trained model, configuration and tokenizer\n",
    "        model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "        # If we save using the predefined names, we can load using `from_pretrained`\n",
    "        output_model_file = os.path.join(save_path, WEIGHTS_NAME)\n",
    "        output_config_file = os.path.join(save_path, CONFIG_NAME)\n",
    "        torch.save(model_to_save.state_dict(), output_model_file)\n",
    "\n",
    "        config = model_to_save.config\n",
    "        config.__dict__[\"deptype2id\"] = self.deptype2id\n",
    "        config.__dict__[\"polarity2id\"] = self.polarity2id\n",
    "        with open(output_config_file, \"w\", encoding='utf-8') as writer:\n",
    "            writer.write(config.to_json_string())\n",
    "        output_args_file = os.path.join(save_path, 'training_args.bin')\n",
    "        torch.save(args, output_args_file)\n",
    "        subprocess.run(['cp', self.vocab_path, os.path.join(save_path, 'vocab.txt')])\n",
    "\n",
    "    def _train(self, criterion, optimizer, train_data_loader, val_data_loader, test_data_loader):\n",
    "        print('function _train starts...')\n",
    "        max_val_acc = -1\n",
    "        max_val_f1 = -1\n",
    "        global_step = 0\n",
    "        path = None\n",
    "\n",
    "        model_home = self.opt.model_path \n",
    "#         model_home += '-' + strftime(\"%y%m%d-%H%M\", localtime())\n",
    "\n",
    "        results = {\"bert_model\": self.opt.bert_model, \"batch_size\": self.opt.batch_size,\n",
    "                   \"learning_rate\": self.opt.learning_rate, \"seed\": self.opt.seed,\n",
    "                  \"num_epoch\": self.opt.num_epoch, \"l2reg\": self.opt.l2reg,\n",
    "                  \"dropout\": self.opt.dropout}\n",
    "        for epoch in range(self.opt.num_epoch):\n",
    "            logger.info('>' * 100)\n",
    "            logger.info('epoch: {}'.format(epoch))\n",
    "            n_correct, n_total, loss_total = 0, 0, 0\n",
    "            \n",
    "            self.model.train() \n",
    "            \n",
    "            for i_batch, t_sample_batched in enumerate(train_data_loader):\n",
    "\n",
    "                global_step += 1\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                n_correct, n_total, loss_total = self.train_step(optimizer, i_batch, t_sample_batched, criterion, n_correct, n_total, loss_total)\n",
    "                if global_step % self.opt.log_step == 0:\n",
    "                    train_acc = n_correct / n_total\n",
    "                    train_loss = loss_total / n_total\n",
    "                    logger.info('epoch: {}, loss: {:.4f}, train acc: {:.4f}'.format(epoch, train_loss, train_acc))\n",
    "\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache() # try optimization\n",
    "                \n",
    "            \n",
    "            #OLD LINES\n",
    "            val_acc, val_f1 = Instructor._evaluate_acc_f1(self.model, val_data_loader, device=self.opt.device)\n",
    "            logger.info('>epoch: {}, val_acc: {:.4f}, val_f1: {:.4f}'.format(epoch, val_acc, val_f1))\n",
    "            results[\"{}_val_acc\".format(epoch)] = val_acc\n",
    "            results[\"{}_val_f1\".format(epoch)] = val_f1\n",
    "            saving_path = os.path.join(model_home, \"epoch_{}\".format(epoch))\n",
    "            \n",
    "            if not os.path.exists(saving_path):\n",
    "                os.makedirs(saving_path)\n",
    "            if val_acc > max_val_acc or (val_acc == max_val_acc and val_f1 > max_val_f1):\n",
    "                max_val_acc = val_acc\n",
    "                max_val_f1 = val_f1\n",
    "                \n",
    "                if opt.save_models == 'last':\n",
    "                    best_path = saving_path\n",
    "                    best_model = self.model\n",
    "                elif opt.save_models == 'all':\n",
    "                    self.save_model(saving_path, self.model, self.opt)\n",
    "                elif opt.save_models == 'none':\n",
    "                    pass \n",
    "\n",
    "#                 self.model.eval() #old part we don't need that because inside _evaluate_acc_f1 we have model.eval()\n",
    "                saving_path = os.path.join(model_home, \"epoch_{}_eval.txt\".format(epoch))\n",
    "                test_acc, test_f1 = self._evaluate_acc_f1(self.model, test_data_loader, device=self.opt.device,\n",
    "                                                          saving_path=saving_path)\n",
    "                logger.info('>> epoch: {}, test_acc: {:.4f}, test_f1: {:.4f}'.format(epoch, test_acc, test_f1))\n",
    "\n",
    "                results[\"max_val_acc\"] = max_val_acc\n",
    "                results[\"test_acc\"] = test_acc\n",
    "                results[\"test_f1\"] = test_f1\n",
    "            \n",
    "            output_eval_file = os.path.join(model_home, \"eval_results.txt\")\n",
    "            \n",
    "            with open(output_eval_file, \"w\") as writer:\n",
    "                for k,v in results.items():\n",
    "                    writer.write(\"{}={}\\n\".format(k,v))\n",
    "        \n",
    "        acc_file = os.path.join(model_home, \"acc-{:.4f}\".format(test_acc))\n",
    "        \n",
    "        if opt.save_models == 'last':\n",
    "            self.save_model(best_path, best_model, self.opt)\n",
    "        \n",
    "        with open(acc_file, 'w') as f:\n",
    "            f.write(f\"accuracy: {test_acc}\")\n",
    "        return max_val_acc, test_acc, test_f1\n",
    "    \n",
    "    def train_step(self, optimizer, i_batch, t_sample_batched, criterion, n_correct, n_total, loss_total):\n",
    "        # t_sample_batched[\"raw_text\"],\n",
    "        outputs = self.model(t_sample_batched[\"input_ids\"].to(self.opt.device),\n",
    "                             t_sample_batched[\"segment_ids\"].to(self.opt.device),\n",
    "                             t_sample_batched[\"valid_ids\"].to(self.opt.device),\n",
    "                             t_sample_batched[\"mem_valid_ids\"].to(self.opt.device),\n",
    "                             t_sample_batched[\"dep_adj_matrix\"].to(self.opt.device),\n",
    "                             t_sample_batched[\"dep_value_matrix\"].to(self.opt.device),\n",
    "                             t_sample_batched[\"dep_adj_matrix_knogcn\"].to(self.opt.device),\n",
    "                             t_sample_batched[\"dep_value_matrix_knogcn\"].to(self.opt.device))\n",
    "        targets = t_sample_batched['polarity'].to(self.opt.device)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        n_correct += (torch.argmax(outputs, -1) == targets).sum().item()\n",
    "        n_total += len(outputs)\n",
    "        loss_total += loss.item() * len(outputs)\n",
    "\n",
    "        return n_correct, n_total, loss_total\n",
    "\n",
    "    @staticmethod\n",
    "    def _evaluate_acc_f1(model, data_loader, device, saving_path=None):\n",
    "        model.eval()\n",
    "        \n",
    "        n_correct, n_total = 0, 0\n",
    "        t_targets_all, t_outputs_all = None, None\n",
    "        \n",
    "        #model.eval() #the old place\n",
    "\n",
    "        saving_path_f = open(saving_path, 'w') if saving_path is not None else None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for t_batch, t_sample_batched in enumerate(data_loader):\n",
    "                t_targets = t_sample_batched['polarity'].to(device)\n",
    "                t_raw_texts = t_sample_batched['raw_text']\n",
    "                t_aspects = t_sample_batched['aspect']\n",
    "\n",
    "                t_outputs = model(t_sample_batched[\"input_ids\"].to(device),\n",
    "                                  t_sample_batched[\"segment_ids\"].to(device),\n",
    "                                  t_sample_batched[\"valid_ids\"].to(device),\n",
    "                                  t_sample_batched[\"mem_valid_ids\"].to(device),\n",
    "                                  t_sample_batched[\"dep_adj_matrix\"].to(device),\n",
    "                                  t_sample_batched[\"dep_value_matrix\"].to(device),\n",
    "                                  t_sample_batched[\"dep_adj_matrix_knogcn\"].to(device),\n",
    "                                  t_sample_batched[\"dep_value_matrix_knogcn\"].to(device))\n",
    "                \n",
    "                n_correct += (torch.argmax(t_outputs, -1) == t_targets).sum().item()\n",
    "                n_total += len(t_outputs)\n",
    "\n",
    "                if t_targets_all is None:\n",
    "                    t_targets_all = t_targets\n",
    "                    t_outputs_all = t_outputs\n",
    "                else:\n",
    "                    t_targets_all = torch.cat((t_targets_all, t_targets), dim=0)\n",
    "                    t_outputs_all = torch.cat((t_outputs_all, t_outputs), dim=0)\n",
    "\n",
    "                if saving_path_f is not None:\n",
    "                    for t_target, t_output, t_raw_text, t_aspect in zip(t_targets.detach().cpu().numpy(),\n",
    "                                                                        torch.argmax(t_outputs, -1).detach().cpu().numpy(),\n",
    "                                                                        t_raw_texts, t_aspects):\n",
    "                        saving_path_f.write(\"{}\\t{}\\t{}\\t{}\\n\".format(t_target, t_output, t_raw_text, t_aspect))\n",
    "        acc = n_correct / n_total\n",
    "        f1 = metrics.f1_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average='macro', zero_division=0)\n",
    "        return acc, f1\n",
    "\n",
    "    def train(self):\n",
    "        # Loss and Optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        _params = filter(lambda p: p.requires_grad, self.model.parameters())\n",
    "        optimizer = torch.optim.Adam(_params, lr=self.opt.learning_rate, weight_decay=self.opt.l2reg)\n",
    "\n",
    "        train_data_loader = DataLoader(dataset=self.trainset, batch_size=self.opt.batch_size, shuffle=True)\n",
    "        test_data_loader = DataLoader(dataset=self.testset, batch_size=self.opt.batch_size, shuffle=True) \n",
    "        val_data_loader = DataLoader(dataset=self.valset, batch_size=self.opt.batch_size, shuffle=True) \n",
    "        full_train_data_loader = DataLoader(dataset = self.fulltrainset, batch_size = self.opt.batch_size, shuffle=True)\n",
    "\n",
    "        self._reset_params()\n",
    "        max_val_acc, test_acc, test_f1 = self._train(criterion, optimizer, train_data_loader, val_data_loader, test_data_loader)\n",
    "        return max_val_acc, test_acc, test_f1\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "745ad255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(opt):\n",
    "    logger.info(opt)\n",
    "    config = BertConfig.from_json_file(os.path.join(opt.model_path, CONFIG_NAME))\n",
    "    logger.info(config)\n",
    "\n",
    "    tokenizer = Tokenizer4Bert(opt.max_seq_len, opt.model_path)\n",
    "    if opt.model_type == 'tgcn':\n",
    "        model = AsaTgcn.from_pretrained(opt.model_path)\n",
    "    elif opt.model_type == 'tgcn+sem':\n",
    "        model = AsaTgcnSem.from_pretrained(opt.model_path)\n",
    "    model.set_dropout(opt.dropout)\n",
    "    model.to(opt.device)\n",
    "\n",
    "    deptype2id = config.deptype2id\n",
    "    logger.info(deptype2id)\n",
    "    testset = ABSADataset(opt.test_file, tokenizer, opt, deptype2id=deptype2id)\n",
    "    test_data_loader = DataLoader(dataset=testset, batch_size=opt.batch_size, shuffle=False)\n",
    "    test_acc, test_f1 = Instructor._evaluate_acc_f1(model, test_data_loader, device=opt.device)\n",
    "    logger.info('>> test_acc: {:.4f}, test_f1: {:.4f}'.format(test_acc, test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4627de6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args(model_type = 'tgcn', # tgcn, tgcn+sem, tri_gcn\n",
    "             # Select which modules to use for hybrid model\n",
    "             tgcn = True,\n",
    "             semgcn = True, \n",
    "             lexgcn = True,\n",
    "             knogcn = True,\n",
    "             tgcn_layers = 2,\n",
    "             semgcn_layers = 2,\n",
    "             lexgcn_layers = 2,\n",
    "             knogcn_layers = 2,\n",
    "             path = None, \n",
    "             year='2015',\n",
    "             val_file='val.txt',\n",
    "             log = 'log',\n",
    "             bert_model='bert-large-uncased', # (change underscore to the dash)\n",
    "             #bert_model='bert_large_uncased',\n",
    "             cooc_path = 'cooc_matrix_final2.csv', # Path to co-occurrence matrix file\n",
    "             cooc = None, # Pandas DataFrame co-occurrence matrix. If not specified, it will be loaded from cooc_path\n",
    "             onto_words=None,\n",
    "             onto_words_path='test_ontology_keys.csv',\n",
    "             learning_rate=2e-5,\n",
    "             dropout=0.2,\n",
    "             concat_dropout = 0.4,\n",
    "             bert_dropout=0.2,\n",
    "             l2reg=0.01,\n",
    "             num_epoch=50,\n",
    "             batch_size=6, \n",
    "             log_step=100,\n",
    "             max_seq_len=100,\n",
    "             polarities_dim=3,\n",
    "             device='cuda',\n",
    "             seed=50,\n",
    "             valset_ratio=0.2, # the percentage fo the validation set\n",
    "             do_train=True,\n",
    "             do_eval=True,\n",
    "             eval_epoch_num=0,\n",
    "             fusion_type = 'concat', # 'concat' or 'gate'\n",
    "             use_ensemble = True, \n",
    "            save_models='last',\n",
    "            print_sentences = False, #changed to check the results\n",
    "             optim = 'adam'\n",
    "            ):\n",
    "    \n",
    "    assert model_type == 'tgcn' or model_type == 'tgcn+sem' or model_type == 'tri_gcn'\n",
    "    \n",
    "    opt = argparse.Namespace()\n",
    "    opt.model_type = model_type\n",
    "    opt.modules = {'tgcn': tgcn, 'semgcn': semgcn, 'lexgcn': lexgcn, 'knogcn': knogcn}\n",
    "    opt.num_layers = {'tgcn': tgcn_layers, 'semgcn': semgcn_layers, 'lexgcn': lexgcn_layers, 'knogcn': knogcn_layers}\n",
    "    \n",
    "    opt.year = year\n",
    "    \n",
    "    fusion = \"\" if model_type == 'tgcn' else \"+\" + fusion_type\n",
    "    opt.train_file = f'data/train{year}restaurant.txt'\n",
    "    opt.test_file = f'data/test{year}restaurant.txt'\n",
    "    opt.model_path = f'test_models/{year}{model_type}{fusion}_seed{seed}_reg{l2reg}_drop{dropout}_cdrop{concat_dropout}_lr{learning_rate}_tgcn{tgcn}_semgcn{semgcn}_lexgcn{lexgcn}_knogcn{knogcn}_epochs{num_epoch}_{optim.lower()}'\n",
    "#     if model_type == 'tgcn':\n",
    "#         opt.model_path = f'models/rest_{year}/BERT.L_seed{seed}_reg{l2reg}_drop{dropout}_lr{learning_rate}_epochs{num_epoch}' \n",
    "#     elif model_type == 'tgcn+sem':\n",
    "#         opt.model_path = f'models/rest_{year}/{model_type}/{model_type}_seed{seed}_reg{l2reg}_drop{dropout}_lr{learning_rate}_epochs{num_epoch}'\n",
    "    if do_eval and not do_train:\n",
    "        opt.model_path += f'/epoch_{eval_epoch_num}'\n",
    "    if path:\n",
    "        opt.model_path = path\n",
    "    opt.val_file = val_file\n",
    "    opt.log = log\n",
    "    opt.bert_model = bert_model\n",
    "    opt.cooc_path = cooc_path\n",
    "    opt.cooc = cooc\n",
    "    opt.onto_words=onto_words\n",
    "    opt.onto_words_path=onto_words_path\n",
    "    opt.learning_rate = learning_rate\n",
    "    opt.dropout = dropout\n",
    "    opt.concat_dropout = concat_dropout\n",
    "    opt.bert_dropout = bert_dropout\n",
    "    opt.l2reg = l2reg\n",
    "    opt.num_epoch = num_epoch\n",
    "    opt.batch_size = batch_size\n",
    "    opt.log_step = log_step\n",
    "    opt.max_seq_len = max_seq_len\n",
    "    opt.polarities_dim = polarities_dim\n",
    "    opt.device = device\n",
    "    opt.seed = seed\n",
    "    opt.valset_ratio = valset_ratio\n",
    "    opt.do_train = do_train\n",
    "    opt.do_eval = do_eval\n",
    "    opt.eval_epoch_num = eval_epoch_num\n",
    "    opt.fusion_type = fusion_type\n",
    "    opt.use_ensemble = True\n",
    "    opt.save_models = save_models\n",
    "    opt.print_sent = print_sentences\n",
    "    opt.optim = optim\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2170d8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(opt):\n",
    "    if opt.seed is not None:\n",
    "        random.seed(opt.seed)\n",
    "        np.random.seed(opt.seed)\n",
    "        torch.manual_seed(opt.seed)\n",
    "        torch.cuda.manual_seed(opt.seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2c7a7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(opt):\n",
    "    opt = opt\n",
    "    set_seed(opt)\n",
    "\n",
    "    opt.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \\\n",
    "        if opt.device is None else torch.device(opt.device)\n",
    "    opt.n_gpu = torch.cuda.device_count()\n",
    "\n",
    "    if not os.path.exists(opt.log):\n",
    "        os.makedirs(opt.log)\n",
    "\n",
    "    log_file = '{}/log-{}.log'.format(opt.log, strftime(\"%y%m%d-%H%M\", localtime()))\n",
    "    logger.addHandler(logging.FileHandler(log_file))\n",
    "    \n",
    "    print('strat of the do train...')\n",
    "    if opt.do_train:\n",
    "        ins = Instructor(opt)\n",
    "        max_val_acc, test_acc, test_f1 = ins.train()\n",
    "    elif opt.do_eval:\n",
    "        test(opt)\n",
    "    \n",
    "    return max_val_acc, test_acc, test_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5406b261",
   "metadata": {},
   "source": [
    "# Run program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a0f96061",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FULL COOC MATRIX THAT NEEDED TO BE USED IN THE CODE\n",
    "cooc = pd.read_csv('cooc_matrix_final2.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f775fe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "onto_words =pd.read_csv('test_ontology_keys.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0987eb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt from cache at C:\\Users\\bromi\\.cache\\torch\\pytorch_transformers\\9b3c03a36e83b13d5ba95ac965c9f9074a99e14340c523ab405703179e79fc46.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "#NEW CODE TO AVOID SEVERAL PARAMETERS\n",
    "opt = get_args(batch_size = 6 # CHANGED BECAUSE OF THE MEMORY ERROR\n",
    "                  ,num_epoch = 12, model_type = 'tri_gcn', save_models = 'none', fusion_type = 'concat',\n",
    "                   tgcn = True, semgcn = True, lexgcn = False, knogcn=False, # changed from True\n",
    "                   use_ensemble = False,\n",
    "                  tgcn_layers = 2, semgcn_layers = 2, lexgcn_layers = 2, knogcn_layers = 2, optim = 'adam')\n",
    "deptype2id = ABSADataset.load_deptype_map(opt)\n",
    "tokenizer = Tokenizer4Bert(opt.max_seq_len, opt.bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "434e0312",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(onto_words.shape[0]):\n",
    "    for j in range(onto_words.shape[1]):\n",
    "        word=(onto_words.iloc[i][j])\n",
    "        if type(word)==str:\n",
    "            tokenized_word=tokenizer.tokenizer.convert_tokens_to_ids(tokenizer.tokenizer.tokenize(word))[0]\n",
    "            onto_words.iloc[i][j]=tokenized_word\n",
    "        if type(word)==float:\n",
    "            onto_words.iloc[i][j]=-1 #word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "546582a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "onto_words = [item for sublist in onto_words.values.tolist() for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f17fc3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "onto_words=list(dict.fromkeys(onto_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a33c9d",
   "metadata": {},
   "source": [
    "Hyperparameter searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0d6a4860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "97c5ee4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# csv_file = 'final_results.csv'\n",
    "\n",
    "# res = {'2015': [], '2016': []}\n",
    "# torch.cuda.empty_cache() # try optimization\n",
    "\n",
    "# for i in range(20):\n",
    "#     print('{} loop'.format(i))\n",
    "    \n",
    "#     lr = np.random.choice(np.logspace(-6, -3))\n",
    "#     d = np.random.choice([0.1, 0.2, 0.4])\n",
    "#     cdrop = np.random.choice([0.1, 0.2, 0.4])\n",
    "#     w_decay = np.random.choice(np.logspace(-5, -3))\n",
    "#     seed = np.random.randint(1000)\n",
    "    \n",
    "#     year = '2016'\n",
    "#     opt = get_args(batch_size = 6 #CHANGED FROM 16 BECAUSE OF THE MEMORY ERROR\n",
    "#                    , seed = seed, dropout = d,\n",
    "#                   l2reg = w_decay, learning_rate = lr, year = year,\n",
    "#                   num_epoch = 15, model_type = 'tri_gcn', save_models = 'none', fusion_type = 'concat',\n",
    "#                   concat_dropout = cdrop, cooc = cooc, onto_words = onto_words,\n",
    "#                     tgcn = True, semgcn = True, lexgcn = True, knogcn = True, use_ensemble = False,\n",
    "#                   tgcn_layers = 2, semgcn_layers = 2, lexgcn_layers = 2, knogcn_layers = 2, optim = 'adam')\n",
    "    \n",
    "#     opt.device = torch.device('cuda')\n",
    "    \n",
    "#     max_val_acc, test_acc, test_f1 = main(opt)\n",
    "    \n",
    "#     res[year].append((opt, max_val_acc, test_acc, test_f1))\n",
    "    \n",
    "#     try:\n",
    "#         with open(csv_file, 'a', newline='') as file:\n",
    "#                 writer = csv.writer(file)\n",
    "#                 writer.writerow((year, max_val_acc, test_acc, test_f1, seed, lr, d, cdrop, w_decay))\n",
    "#     finally:\n",
    "#         print('FINISH')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bc0ed2",
   "metadata": {},
   "source": [
    "Training with best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4de9b9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbce8c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loop\n",
      "strat of the do train...\n",
      "Namespace(batch_size=4, bert_dropout=0.2, bert_model='bert-large-uncased', concat_dropout=0.2285714285714286, cooc=          13325          2013      3025      8466          2023      2109  \\\n",
      "13325  0.000000  8.880995e-04  0.021739  0.200000  0.000000e+00  0.000000   \n",
      "2013   0.000888  2.488321e-12  0.000097  0.000355  2.238906e-05  0.000013   \n",
      "3025   0.021739  9.653255e-05  0.000000  0.008696  3.425283e-05  0.000308   \n",
      "8466   0.200000  3.552398e-04  0.008696  0.000000  2.100840e-04  0.002837   \n",
      "2023   0.000000  2.238906e-05  0.000034  0.000210  2.282724e-12  0.000030   \n",
      "...         ...           ...       ...       ...           ...       ...   \n",
      "7108   0.000000  0.000000e+00  0.000000  0.000000  0.000000e+00  0.000000   \n",
      "28297  0.000000  0.000000e+00  0.000000  0.000000  0.000000e+00  0.000000   \n",
      "29425  0.000000  0.000000e+00  0.000000  0.000000  0.000000e+00  0.000000   \n",
      "19337  0.000000  0.000000e+00  0.000000  0.000000  0.000000e+00  0.000000   \n",
      "28946  0.000000  0.000000e+00  0.000000  0.000000  0.000000e+00  0.000000   \n",
      "\n",
      "           2000      2022      1037      2204  ...  24251  5251  22031  12796  \\\n",
      "13325  0.000000  0.000000  0.000000  0.000000  ...    0.0   0.0    0.0    0.0   \n",
      "2013   0.000012  0.000008  0.000018  0.000004  ...    0.0   0.0    0.0    0.0   \n",
      "3025   0.000018  0.000000  0.000015  0.000000  ...    0.0   0.0    0.0    0.0   \n",
      "8466   0.000085  0.000000  0.000027  0.000000  ...    0.0   0.0    0.0    0.0   \n",
      "2023   0.000014  0.000017  0.000012  0.000011  ...    0.0   0.0    0.0    0.0   \n",
      "...         ...       ...       ...       ...  ...    ...   ...    ...    ...   \n",
      "7108   0.000141  0.000000  0.000000  0.000000  ...    0.0   0.0    0.0    0.0   \n",
      "28297  0.000000  0.000000  0.000000  0.000000  ...    0.0   0.0    0.0    0.0   \n",
      "29425  0.000000  0.000000  0.000000  0.000000  ...    0.0   0.0    0.0    0.0   \n",
      "19337  0.000000  0.000000  0.000136  0.000000  ...    0.0   0.0    0.0    0.0   \n",
      "28946  0.000000  0.000000  0.000000  0.000000  ...    0.0   0.0    0.0    0.0   \n",
      "\n",
      "          16838  7108  28297  29425  19337  28946  \n",
      "13325  0.000000   0.0    0.0    0.0    0.0    0.0  \n",
      "2013   0.000000   0.0    0.0    0.0    0.0    0.0  \n",
      "3025   0.000000   0.0    0.0    0.0    0.0    0.0  \n",
      "8466   0.000000   0.0    0.0    0.0    0.0    0.0  \n",
      "2023   0.000525   0.0    0.0    0.0    0.0    0.0  \n",
      "...         ...   ...    ...    ...    ...    ...  \n",
      "7108   0.000000   0.0    0.0    0.0    0.0    0.0  \n",
      "28297  0.000000   0.0    0.0    0.0    0.0    0.0  \n",
      "29425  0.000000   0.0    0.0    0.0    0.0    0.0  \n",
      "19337  0.000000   0.0    0.0    0.0    0.0    0.0  \n",
      "28946  0.000000   0.0    0.0    0.0    0.0    0.0  \n",
      "\n",
      "[10655 rows x 10655 columns], cooc_path='cooc_matrix_final2.csv', device=device(type='cuda'), do_eval=True, do_train=True, dropout=0.2285714285714286, eval_epoch_num=0, fusion_type='concat', l2reg=0.027059715881067578, learning_rate=1.1122448979591838e-05, log='log', log_step=100, max_seq_len=100, model_path='test_models/2015tri_gcn+concat_seed65_reg0.027059715881067578_drop0.2285714285714286_cdrop0.2285714285714286_lr1.1122448979591838e-05_tgcnTrue_semgcnTrue_lexgcnTrue_knogcnTrue_epochs15_adam', model_type='tri_gcn', modules={'tgcn': True, 'semgcn': True, 'lexgcn': True, 'knogcn': True}, n_gpu=1, num_epoch=15, num_layers={'tgcn': 3, 'semgcn': 2, 'lexgcn': 2, 'knogcn': 2}, onto_words=[10733, -1, 15453, 3208, 24799, 8013, 7222, 9805, 4189, 2173, 3976, 20874, 2204, 5481, 8808, 6265, 8796, 10140, 2058, 1002, 5580, 5127, 15184, 17021, 2111, 3962, 20857, 14736, 4310, 18765, 2627, 8752, 16222, 2088, 5572, 3532, 3347, 19739, 12901, 2502, 2293, 4895, 11519, 8242, 2572, 7224, 4292, 21209, 11007, 4276, 11813, 4654, 8312, 7954, 4113, 17227, 3058, 10305, 2039, 25545, 4664, 6581, 3291, 2833, 2431, 16521, 14894, 2862, 3643, 3066, 24570, 19960, 2155, 2658, 3407, 4157, 9525, 3528, 14255, 2307, 25732, 2153, 22293, 11536, 2217, 4166, 6314, 19424, 2388, 2002, 2152, 10514, 2053, 20209, 27940, 2310, 5724, 4024, 16755, 28667, 5909, 9202, 4690, 13207, 5622, 4382, 8633, 2237, 11737, 24480, 4840, 29593, 3295, 3677, 22566, 12726, 6240, 12559, 23566, 4487, 9364, 15640, 11565, 10439, 18783, 26927, 3178, 2795, 3768, 3193, 7746, 23601, 2344, 14469, 6450, 12832, 8974, 4392, 6842, 7505, 4474, 2465, 5355, 2352, 7759, 3737, 12003, 24665, 16286, 9608, 5404, 3129, 11840, 4550, 10168, 15610, 13877, 18074, 18064, 2648, 7254, 19350, 2779, 6513, 5227, 6898, 8052, 3496, 3601, 14380, 2300, 4318, 2047, 6396, 16392, 2190, 9686, 4671, 11079, 7987, 20130, 3147, 1999, 3942, 6090, 2181, 2665, 5891, 3021, 1045, 2033, 6369, 12087, 4440, 15653, 9587, 2017, 3124, 4847, 3529, 20316, 4989, 4658, 2205, 24654, 12183, 6429, 5371, 4825, 3796, 2160, 2016, 2607, 2057, 2149, 10520, 5101, 5440, 5785, 8782, 2796, 11350, 10026, 12090, 6686, 6207, 19154, 4138, 23766, 14437, 5409, 3959, 5151, 2919, 6649, 5069, 2314, 8235, 21475, 26931, 4596, 3371, 4524, 3325, 6298, 5697, 5699, 6949, 2640, 2769, 15212, 10392, 2282, 7668, 9788, 8739, 15415, 3514, 4866, 9850, 21774, 13558, 7967, 6959, 2214, 3869, 2835, 11424, 6231, 3095, 3626, 3697, 3954, 17113, 2986, 2235, 6805, 8562, 5510, 18901, 18066, 8288, 11642, 12476, 4372, 11112, 2822, 14044, 11937, 4512, 6919, 9841, 8117, 23621, 2395, 22241, 2887, 28774, 7975, 3968, 3413, 2449, 5875, 7273, 9610, 28253, 2012, 4897, 25482, 4511, 2503, 2027, 2068, 2103, 19404, 2161, 12465, 2269, 6911, 11951, 5787, 2051, 22766, 24857, 5959, 2767, 5474, 10036, 2364, 11134, 5541, 3835, 22249, 3819, 9643, 4569, 21871, 12327, 7128, 20856, 4306, 2326, 5379, 13352, 3927, 13642, 3871, 5955, 2189, 8639, 19116], onto_words_path='test_ontology_keys.csv', optim='adam', polarities_dim=3, print_sent=False, save_models='none', seed=65, test_file='data/test2015restaurant.txt', train_file='data/train2015restaurant.txt', use_ensemble=True, val_file='val.txt', valset_ratio=0.2, year='2015')\n",
      "{'none': 0, 'acl': 1, 'acl:relcl': 2, 'advcl': 3, 'advcl:relcl': 4, 'advmod': 5, 'amod': 6, 'appos': 7, 'aux': 8, 'aux:pass': 9, 'case': 10, 'cc': 11, 'cc:preconj': 12, 'ccomp': 13, 'compound': 14, 'compound:prt': 15, 'conj': 16, 'cop': 17, 'csubj': 18, 'dep': 19, 'det': 20, 'det:predet': 21, 'discourse': 22, 'expl': 23, 'fixed': 24, 'flat': 25, 'iobj': 26, 'list': 27, 'mark': 28, 'nmod': 29, 'nmod:npmod': 30, 'nmod:poss': 31, 'nmod:tmod': 32, 'nsubj': 33, 'nsubj:outer': 34, 'nsubj:pass': 35, 'nummod': 36, 'obj': 37, 'obl': 38, 'obl:agent': 39, 'obl:npmod': 40, 'obl:tmod': 41, 'parataxis': 42, 'punct': 43, 'reparandum': 44, 'root': 45, 'vocative': 46, 'xcomp': 47}\n",
      "{'-1': 0, '0': 1, '1': 2}\n",
      "loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt from cache at C:\\Users\\bromi\\.cache\\torch\\pytorch_transformers\\9b3c03a36e83b13d5ba95ac965c9f9074a99e14340c523ab405703179e79fc46.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-config.json from cache at C:\\Users\\bromi\\.cache\\torch\\pytorch_transformers\\6dfaed860471b03ab5b9acb6153bea82b6632fb9bbe514d3fff050fe1319ee6d.788fed32bb8481a9b15ce726d41c53d5d5066b04c667e34ce3a7a3826d1573d8\n",
      "Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "{\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 3,\n",
      "  \"num_types\": 48,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check the type of the model...\n",
      "loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-pytorch_model.bin from cache at C:\\Users\\bromi\\.cache\\torch\\pytorch_transformers\\54da47087cc86ce75324e4dc9bbb5f66c6e83a7c6bd23baea8b489acc8d09aa4.4d5343a4b979c4beeaadef17a0453d1bb183dd9b084f58b84c7cc781df343ae6\n",
      "Weights of AsaTgcnSem not initialized from pretrained model: ['gate_weight', 'gate_bias', 'ensemble', 'TGCNLayers.0.weight', 'TGCNLayers.0.bias', 'TGCNLayers.0.dense.weight', 'TGCNLayers.1.weight', 'TGCNLayers.1.bias', 'TGCNLayers.1.dense.weight', 'TGCNLayers.2.weight', 'TGCNLayers.2.bias', 'TGCNLayers.2.dense.weight', 'SemGCNLayers.0.weight', 'SemGCNLayers.0.bias', 'SemGCNLayers.1.weight', 'SemGCNLayers.1.bias', 'LexGCNLayers.0.weight', 'LexGCNLayers.0.bias', 'LexGCNLayers.1.weight', 'LexGCNLayers.1.bias', 'KnoGCNLayers.0.weight', 'KnoGCNLayers.0.bias', 'KnoGCNLayers.0.dense.weight', 'KnoGCNLayers.1.weight', 'KnoGCNLayers.1.bias', 'KnoGCNLayers.1.dense.weight', 'fc_single.weight', 'fc_single.bias', 'ensemble_linear_tgcn.weight', 'ensemble_linear_tgcn.bias', 'ensemble_linear_semgcn.weight', 'ensemble_linear_semgcn.bias', 'ensemble_linear_lexgcn.weight', 'ensemble_linear_lexgcn.bias', 'ensemble_linear_knogcn.weight', 'ensemble_linear_knogcn.bias', 'dep_embedding.weight']\n",
      "Weights from pretrained model not used in AsaTgcnSem: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "downloading the files...\n",
      "[{'input_ids': tensor([  101, 13325,  2013,  3025,  8466,  2023,  2109,  2000,  2022,  1037,\n",
      "         2204,  2173,  1010,  2021,  2025,  2151,  2936,  1012,   102,  2173,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'valid_ids': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'segment_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'mem_valid_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'dep_adj_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_value_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_adj_matrix_knogcn': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_value_matrix_knogcn': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'polarity': 0, 'raw_text': 'judging from previous posts this used to be a good place , but not any longer .', 'aspect': 'place'}]\n",
      "[{'input_ids': tensor([  101, 13325,  2013,  3025,  8466,  2023,  2109,  2000,  2022,  1037,\n",
      "         2204,  2173,  1010,  2021,  2025,  2151,  2936,  1012,   102,  2173,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'valid_ids': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'segment_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'mem_valid_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'dep_adj_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_value_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_adj_matrix_knogcn': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_value_matrix_knogcn': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'polarity': 0, 'raw_text': 'judging from previous posts this used to be a good place , but not any longer .', 'aspect': 'place'}]\n",
      "[{'input_ids': tensor([ 101, 2293, 2632, 4487, 2474,  102, 2632, 4487, 2474,  102,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0]), 'valid_ids': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'segment_ids': tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'mem_valid_ids': tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'dep_adj_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_value_matrix': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_adj_matrix_knogcn': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'dep_value_matrix_knogcn': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'polarity': 2, 'raw_text': 'love al di la ', 'aspect': 'al di la'}]\n",
      "check if the val exist...\n",
      "check device opt.device.type == cuda\n",
      "opt.device.type == cuda\n",
      "cuda memory allocated: 1408779264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function _train starts...\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 0\n",
      "epoch: 0, loss: 0.8070, train acc: 0.6950\n",
      "epoch: 0, loss: 0.7198, train acc: 0.7262\n",
      ">epoch: 0, val_acc: 0.7490, val_f1: 0.2966\n",
      ">> epoch: 0, test_acc: 0.6064, test_f1: 0.2972\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 1\n",
      "epoch: 1, loss: 0.4597, train acc: 0.8295\n",
      "epoch: 1, loss: 0.4410, train acc: 0.8472\n",
      "epoch: 1, loss: 0.4123, train acc: 0.8545\n",
      ">epoch: 1, val_acc: 0.8627, val_f1: 0.5603\n",
      ">> epoch: 1, test_acc: 0.8208, test_f1: 0.5758\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 2\n",
      "epoch: 2, loss: 0.2850, train acc: 0.8977\n",
      "epoch: 2, loss: 0.2471, train acc: 0.9136\n",
      ">epoch: 2, val_acc: 0.9059, val_f1: 0.5917\n",
      ">> epoch: 2, test_acc: 0.7990, test_f1: 0.5550\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 3\n",
      "epoch: 3, loss: 0.1101, train acc: 0.9844\n",
      "epoch: 3, loss: 0.1791, train acc: 0.9337\n",
      "epoch: 3, loss: 0.1868, train acc: 0.9310\n",
      ">epoch: 3, val_acc: 0.9098, val_f1: 0.5943\n",
      ">> epoch: 3, test_acc: 0.8174, test_f1: 0.6256\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 4\n",
      "epoch: 4, loss: 0.1340, train acc: 0.9441\n",
      "epoch: 4, loss: 0.1687, train acc: 0.9403\n",
      ">epoch: 4, val_acc: 0.8706, val_f1: 0.6357\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 5\n",
      "epoch: 5, loss: 0.1216, train acc: 0.9375\n",
      "epoch: 5, loss: 0.1976, train acc: 0.9146\n",
      "epoch: 5, loss: 0.2015, train acc: 0.9205\n",
      ">epoch: 5, val_acc: 0.8941, val_f1: 0.5782\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 6\n",
      "epoch: 6, loss: 0.1176, train acc: 0.9531\n",
      "epoch: 6, loss: 0.2330, train acc: 0.9146\n",
      ">epoch: 6, val_acc: 0.8863, val_f1: 0.5695\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 7\n",
      "epoch: 7, loss: 0.5152, train acc: 0.8438\n",
      "epoch: 7, loss: 0.2185, train acc: 0.9259\n",
      "epoch: 7, loss: 0.1863, train acc: 0.9363\n"
     ]
    }
   ],
   "source": [
    "num_trials = 1\n",
    "\n",
    "# lr_space = np.linspace(5e-6, 2e-5, num = 50)\n",
    "# cd_space = np.linspace(0, 0.6, 14)\n",
    "# d_space = np.linspace(0, 0.4, 8)\n",
    "# reg_space = np.logspace(-2.5, -1.2, num=100)\n",
    "\n",
    "for i in range(num_trials):\n",
    "    \n",
    "    print('{} loop'.format(i))\n",
    "    \n",
    "#     c_d = np.random.choice(cd_space)\n",
    "#     d = np.random.choice(d_space)\n",
    "#     reg = np.random.choice(reg_space)\n",
    "#     lr = np.random.choice(lr_space)\n",
    "#     seed = np.random.randint(1000) \n",
    "\n",
    "    c_d=0.2285714285714286\n",
    "    d=0.2285714285714286\n",
    "    reg=0.027059715881067578\n",
    "    lr=1.1122448979591838e-05\n",
    "    seed=65\n",
    "    \n",
    "    fusion_type = 'concat'\n",
    "    opt = get_args(batch_size = 4 \n",
    "                   , seed = seed, dropout = d,\n",
    "              l2reg = reg, learning_rate = lr, year = '2015',\n",
    "              num_epoch = 15, model_type = 'tri_gcn', save_models = 'none', fusion_type = fusion_type, use_ensemble=False,\n",
    "                   cooc=cooc, onto_words=onto_words,\n",
    "              concat_dropout = c_d, tgcn = True, semgcn = True, lexgcn = True, knogcn = True)\n",
    "    main(opt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
